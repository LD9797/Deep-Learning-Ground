{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2: Redes Recurrentes y Representaciones Incrustadas\n",
    "\n",
    "## 2. (30 puntos extra) Perceptrón multi-capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# classifier imports\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the SMS+Spam+Collection\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the dataset using Pandas and delimiter as tabulation.\n",
    "messages = pd.read_csv('.\\smsspamcollection\\SMSSpamCollection', encoding='latin-1',delimiter=\"\\t\",header=None)\n",
    "#Set labels on the colums to ease manipulation.\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Preparing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ham with 0 and spam with 1\n",
    "messages = messages.replace(['ham','spam'],[0, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim implementation for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             text_pp  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                        [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup...  \n",
       "3     [dun, say, so, early, hor, already, then, say]  \n",
       "4  [nah, don, think, he, goes, to, usf, he, lives...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess with built-in Gensim libraries creating a new column with the new pre-processed text.\n",
    "#simple_preprocess lowercases, tokenizes and de-accents and returns the final tokens as unicode strings.\n",
    "#We are calling the pre-processed text, text_pp.\n",
    "messages['text_pp'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x,deacc=True,min_len=2,max_len=15))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To fill NaN values \n",
    "\n",
    "#messages['text_pp'].fillna(value='none', inplace=True)\n",
    "#messages['text'].fillna(value='none', inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages['Count_clean']=0\n",
    "#for i in np.arange(0,len(messages.text_clean)):\n",
    "#    messages.loc[i,'Count_clean'] = len(messages.loc[i,'text_clean'])\n",
    "\n",
    "messages['Count']=0\n",
    "for i in np.arange(0,len(messages.text)):\n",
    "    messages.loc[i,'Count'] = len(messages.loc[i,'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Where 0 is not spam and 1 spam.\n",
    "messages['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of       label                                               text  \\\n",
       "0         0  Go until jurong point, crazy.. Available only ...   \n",
       "1         0                      Ok lar... Joking wif u oni...   \n",
       "2         1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3         0  U dun say so early hor... U c already then say...   \n",
       "4         0  Nah I don't think he goes to usf, he lives aro...   \n",
       "...     ...                                                ...   \n",
       "5567      1  This is the 2nd time we have tried 2 contact u...   \n",
       "5568      0              Will Ã¼ b going to esplanade fr home?   \n",
       "5569      0  Pity, * was in mood for that. So...any other s...   \n",
       "5570      0  The guy did some bitching but I acted like i'd...   \n",
       "5571      0                         Rofl. Its true to its name   \n",
       "\n",
       "                                                text_pp  Count  \n",
       "0     [go, until, jurong, point, crazy, available, o...    111  \n",
       "1                           [ok, lar, joking, wif, oni]     29  \n",
       "2     [free, entry, in, wkly, comp, to, win, fa, cup...    155  \n",
       "3        [dun, say, so, early, hor, already, then, say]     49  \n",
       "4     [nah, don, think, he, goes, to, usf, he, lives...     61  \n",
       "...                                                 ...    ...  \n",
       "5567  [this, is, the, nd, time, we, have, tried, con...    161  \n",
       "5568         [will, a¼, going, to, esplanade, fr, home]     37  \n",
       "5569  [pity, was, in, mood, for, that, so, any, othe...     57  \n",
       "5570  [the, guy, did, some, bitching, but, acted, li...    125  \n",
       "5571                   [rofl, its, true, to, its, name]     26  \n",
       "\n",
       "[5572 rows x 4 columns]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Messages\n",
    "\n",
    "### PorterStemeer to remove stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an empty list to build the corpus for the word2Vec model.\n",
    "corpus = []\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'great', 'world', 'la', 'buffet', 'cine', 'there', 'got', 'amore', 'wat']\n",
      "['ok', 'lar', 'joking', 'wif', 'oni']\n"
     ]
    }
   ],
   "source": [
    "print (messages['text_pp'][0])\n",
    "print (messages['text_pp'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t MESSAGE  0\n",
      "\n",
      " After Stemming - Message  0  :  ['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'great', 'world', 'la', 'buffet', 'cine', 'got', 'amor', 'wat']\n",
      "\n",
      " Final Prepared - Message  0  :  go jurong point crazi avail bugi great world la buffet cine got amor wat \n",
      "\n",
      "\n",
      "\t\t\t\t MESSAGE  1\n",
      "\n",
      " After Stemming - Message  1  :  ['ok', 'lar', 'joke', 'wif', 'oni']\n",
      "\n",
      " Final Prepared - Message  1  :  ok lar joke wif oni \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5572):\n",
    "\n",
    "    \n",
    "    msg = messages['text_pp'][i]\n",
    "    \n",
    "    \n",
    "    if i<2:\n",
    "        print(\"\\t\\t\\t\\t MESSAGE \", i)\n",
    "    \n",
    "\n",
    "    # Stemming with PorterStemmer handling Stop Words\n",
    "    msg = [ps.stem(word) for word in msg if not word in set(stopwords.words('english'))]\n",
    "    if i<2:\n",
    "        print(\"\\n After Stemming - Message \", i, \" : \", msg)\n",
    "    \n",
    "    # preparing Messages with Remaining Tokens\n",
    "    msg = ' '.join(msg)\n",
    "    if i<2:\n",
    "        print(\"\\n Final Prepared - Message \", i, \" : \", msg, \"\\n\\n\")\n",
    "    \n",
    "    # Preparing WordVector Corpus\n",
    "    corpus.append(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for Word2Vec and MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We use the CountVectorizer to convert the collection of text messages to a matrix of token counts. \n",
    "cv = CountVectorizer()\n",
    "#And create our x array that will be used later on the MLP model.\n",
    "x = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "#We built our y, using the labels from the dataset.\n",
    "y = messages['label']\n",
    "#Then transform the labels and prepare y for later use on MLP model.\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "#We will split the dataset in training and testing sets, will be later feed to the dataloder.\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y,test_size= 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model\n",
    "\n",
    "### Build the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=50\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(min_count=2,\n",
    "                     window=3,\n",
    "                     vector_size=num_features,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build word2vec vocabulary with the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(messages['text_pp'], progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(messages['text_pp'], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'and',\n",
       " 'in',\n",
       " 'is',\n",
       " 'me',\n",
       " 'my',\n",
       " 'it',\n",
       " 'for',\n",
       " 'your',\n",
       " 'of',\n",
       " 'call',\n",
       " 'that',\n",
       " 'have',\n",
       " 'on',\n",
       " 'now',\n",
       " 'are',\n",
       " 'can',\n",
       " 'so',\n",
       " 'but',\n",
       " 'not',\n",
       " 'or',\n",
       " 'we',\n",
       " 'do',\n",
       " 'at',\n",
       " 'get',\n",
       " 'ur',\n",
       " 'if',\n",
       " 'will',\n",
       " 'be',\n",
       " 'with',\n",
       " 'no',\n",
       " 'just',\n",
       " 'this',\n",
       " 'gt',\n",
       " 'lt',\n",
       " 'how',\n",
       " 'up',\n",
       " 'when',\n",
       " 'ok',\n",
       " 'what',\n",
       " 'go',\n",
       " 'free',\n",
       " 'from',\n",
       " 'all',\n",
       " 'out',\n",
       " 'll',\n",
       " 'know',\n",
       " 'like',\n",
       " 'good',\n",
       " 'day',\n",
       " 'then',\n",
       " 'am',\n",
       " 'got',\n",
       " 'come',\n",
       " 'there',\n",
       " 'was',\n",
       " 'he',\n",
       " 'its',\n",
       " 'time',\n",
       " 'only',\n",
       " 'love',\n",
       " 'send',\n",
       " 'want',\n",
       " 'text',\n",
       " 'txt',\n",
       " 'as',\n",
       " 'one',\n",
       " 'by',\n",
       " 'going',\n",
       " 'need',\n",
       " 'home',\n",
       " 'she',\n",
       " 'about',\n",
       " 'stop',\n",
       " 'lor',\n",
       " 'sorry',\n",
       " 'today',\n",
       " 'don',\n",
       " 'see',\n",
       " 'still',\n",
       " 'back',\n",
       " 'da',\n",
       " 'our',\n",
       " 'dont',\n",
       " 'reply',\n",
       " 'mobile',\n",
       " 'take',\n",
       " 'hi',\n",
       " 'tell',\n",
       " 'they',\n",
       " 'new',\n",
       " 'later',\n",
       " 'please',\n",
       " 'any',\n",
       " 'her',\n",
       " 'pls',\n",
       " 'did',\n",
       " 'think',\n",
       " 'been',\n",
       " 'phone',\n",
       " 'some',\n",
       " 'week',\n",
       " 'dear',\n",
       " 'here',\n",
       " 'who',\n",
       " 'well',\n",
       " 'a¼',\n",
       " 'where',\n",
       " 'has',\n",
       " 'night',\n",
       " 're',\n",
       " 'much',\n",
       " 'an',\n",
       " 'great',\n",
       " 'oh',\n",
       " 'hope',\n",
       " 'msg',\n",
       " 'claim',\n",
       " 'hey',\n",
       " 'him',\n",
       " 'more',\n",
       " 'too',\n",
       " 'happy',\n",
       " 'wat',\n",
       " 'had',\n",
       " 'give',\n",
       " 'yes',\n",
       " 'way',\n",
       " 'make',\n",
       " 'work',\n",
       " 'www',\n",
       " 've',\n",
       " 'number',\n",
       " 'should',\n",
       " 'message',\n",
       " 'won',\n",
       " 'tomorrow',\n",
       " 'say',\n",
       " 'prize',\n",
       " 'right',\n",
       " 'already',\n",
       " 'after',\n",
       " 'said',\n",
       " 'ask',\n",
       " 'amp',\n",
       " 'cash',\n",
       " 'doing',\n",
       " 'yeah',\n",
       " 'im',\n",
       " 'really',\n",
       " 'why',\n",
       " 'win',\n",
       " 'life',\n",
       " 'them',\n",
       " 'find',\n",
       " 'meet',\n",
       " 'very',\n",
       " 'miss',\n",
       " 'babe',\n",
       " 'let',\n",
       " 'morning',\n",
       " 'thanks',\n",
       " 'last',\n",
       " 'anything',\n",
       " 'cos',\n",
       " 'com',\n",
       " 'would',\n",
       " 'uk',\n",
       " 'lol',\n",
       " 'also',\n",
       " 'nokia',\n",
       " 'care',\n",
       " 'every',\n",
       " 'pick',\n",
       " 'sure',\n",
       " 'min',\n",
       " 'over',\n",
       " 'urgent',\n",
       " 'keep',\n",
       " 'sent',\n",
       " 'something',\n",
       " 'contact',\n",
       " 'again',\n",
       " 'buy',\n",
       " 'gud',\n",
       " 'us',\n",
       " 'cant',\n",
       " 'before',\n",
       " 'wait',\n",
       " 'first',\n",
       " 'box',\n",
       " 'his',\n",
       " 'even',\n",
       " 'someone',\n",
       " 'thing',\n",
       " 'feel',\n",
       " 'went',\n",
       " 'help',\n",
       " 'nice',\n",
       " 'next',\n",
       " 'were',\n",
       " 'off',\n",
       " 'soon',\n",
       " 'which',\n",
       " 'around',\n",
       " 'tone',\n",
       " 'place',\n",
       " 'could',\n",
       " 'service',\n",
       " 'tonight',\n",
       " 'money',\n",
       " 'mins',\n",
       " 'per',\n",
       " 'chat',\n",
       " 'late',\n",
       " 'many',\n",
       " 'sms',\n",
       " 'always',\n",
       " 'customer',\n",
       " 'ya',\n",
       " 'sleep',\n",
       " 'gonna',\n",
       " 'wan',\n",
       " 'leave',\n",
       " 'down',\n",
       " 'co',\n",
       " 'friends',\n",
       " 'pm',\n",
       " 'dun',\n",
       " 'other',\n",
       " 'st',\n",
       " 'wish',\n",
       " 'name',\n",
       " 'told',\n",
       " 'things',\n",
       " 'waiting',\n",
       " 'hello',\n",
       " 'special',\n",
       " 'try',\n",
       " 'may',\n",
       " 'friend',\n",
       " 'fine',\n",
       " 'best',\n",
       " 'coming',\n",
       " 'haha',\n",
       " 'yet',\n",
       " 'guaranteed',\n",
       " 'done',\n",
       " 'same',\n",
       " 'getting',\n",
       " 'thk',\n",
       " 'year',\n",
       " 'people',\n",
       " 'heart',\n",
       " 'ppm',\n",
       " 'use',\n",
       " 'days',\n",
       " 'thought',\n",
       " 'th',\n",
       " 'god',\n",
       " 'holiday',\n",
       " 'person',\n",
       " 'stuff',\n",
       " 'lunch',\n",
       " 'live',\n",
       " 'man',\n",
       " 'class',\n",
       " 'bit',\n",
       " 'smile',\n",
       " 'talk',\n",
       " 'few',\n",
       " 'cs',\n",
       " 'draw',\n",
       " 'being',\n",
       " 'didn',\n",
       " 'never',\n",
       " 'end',\n",
       " 'ill',\n",
       " 'better',\n",
       " 'job',\n",
       " 'yup',\n",
       " 'line',\n",
       " 'trying',\n",
       " 'meeting',\n",
       " 'house',\n",
       " 'cool',\n",
       " 'thats',\n",
       " 'ready',\n",
       " 'finish',\n",
       " 'having',\n",
       " 'mind',\n",
       " 'pobox',\n",
       " 'long',\n",
       " 'car',\n",
       " 'dat',\n",
       " 'half',\n",
       " 'real',\n",
       " 'wk',\n",
       " 'enjoy',\n",
       " 'account',\n",
       " 'yo',\n",
       " 'latest',\n",
       " 'check',\n",
       " 'sir',\n",
       " 'lot',\n",
       " 'chance',\n",
       " 'month',\n",
       " 'play',\n",
       " 'because',\n",
       " 'than',\n",
       " 'bt',\n",
       " 'problem',\n",
       " 'luv',\n",
       " 'eat',\n",
       " 'awarded',\n",
       " 'lar',\n",
       " 'camera',\n",
       " 'word',\n",
       " 'receive',\n",
       " 'wanna',\n",
       " 'po',\n",
       " 'nothing',\n",
       " 'start',\n",
       " 'world',\n",
       " 'guess',\n",
       " 'guys',\n",
       " 'shit',\n",
       " 'liao',\n",
       " 'another',\n",
       " 'big',\n",
       " 'birthday',\n",
       " 'girl',\n",
       " 'shows',\n",
       " 'into',\n",
       " 'dinner',\n",
       " 'xxx',\n",
       " 'sweet',\n",
       " 'ah',\n",
       " 'quite',\n",
       " 'jus',\n",
       " 'pa',\n",
       " 'ever',\n",
       " 'might',\n",
       " 'hrs',\n",
       " 'video',\n",
       " 'room',\n",
       " 'offer',\n",
       " 'watching',\n",
       " 'wont',\n",
       " 'landline',\n",
       " 'cost',\n",
       " 'thanx',\n",
       " 'bed',\n",
       " 'watch',\n",
       " 'probably',\n",
       " 'early',\n",
       " 'tv',\n",
       " 'two',\n",
       " 'called',\n",
       " 'aight',\n",
       " 'speak',\n",
       " 'hear',\n",
       " 'weekend',\n",
       " 'nd',\n",
       " 'once',\n",
       " 'pay',\n",
       " 'forgot',\n",
       " 'actually',\n",
       " 'plan',\n",
       " 'rate',\n",
       " 'sat',\n",
       " 'apply',\n",
       " 'nite',\n",
       " 'boy',\n",
       " 'maybe',\n",
       " 'does',\n",
       " 'minutes',\n",
       " 'princess',\n",
       " 'left',\n",
       " 'bad',\n",
       " 'den',\n",
       " 'shall',\n",
       " 'remember',\n",
       " 'fun',\n",
       " 'ringtone',\n",
       " 'look',\n",
       " 'baby',\n",
       " 'shopping',\n",
       " 'easy',\n",
       " 'made',\n",
       " 'kiss',\n",
       " 'part',\n",
       " 'reach',\n",
       " 'code',\n",
       " 'orange',\n",
       " 'office',\n",
       " 'dunno',\n",
       " 'between',\n",
       " 'age',\n",
       " 'bus',\n",
       " 'anyway',\n",
       " 'hour',\n",
       " 'xx',\n",
       " 'little',\n",
       " 'everything',\n",
       " 'award',\n",
       " 'dis',\n",
       " 'didnt',\n",
       " 'tones',\n",
       " 'face',\n",
       " 'leh',\n",
       " 'selected',\n",
       " 'working',\n",
       " 'network',\n",
       " 'true',\n",
       " 'put',\n",
       " 'fuck',\n",
       " 'enough',\n",
       " 'wife',\n",
       " 'looking',\n",
       " 'dad',\n",
       " 'thank',\n",
       " 'those',\n",
       " 'town',\n",
       " 'texts',\n",
       " 'without',\n",
       " 'while',\n",
       " 'tmr',\n",
       " 'calls',\n",
       " 'afternoon',\n",
       " 'asked',\n",
       " 'years',\n",
       " 'collect',\n",
       " 'evening',\n",
       " 'most',\n",
       " 'missing',\n",
       " 'gift',\n",
       " 'entry',\n",
       " 'though',\n",
       " 'wif',\n",
       " 'pain',\n",
       " 'okay',\n",
       " 'says',\n",
       " 'xmas',\n",
       " 'sexy',\n",
       " 'school',\n",
       " 'details',\n",
       " 'hav',\n",
       " 'wanted',\n",
       " 'important',\n",
       " 'until',\n",
       " 'mail',\n",
       " 'since',\n",
       " 'came',\n",
       " 'join',\n",
       " 'valid',\n",
       " 'times',\n",
       " 'must',\n",
       " 'means',\n",
       " 'goes',\n",
       " 'price',\n",
       " 'wake',\n",
       " 'able',\n",
       " 'collection',\n",
       " 'wot',\n",
       " 'bring',\n",
       " 'guy',\n",
       " 'til',\n",
       " 'missed',\n",
       " 'abt',\n",
       " 'juz',\n",
       " 'plz',\n",
       " 'decimal',\n",
       " 'show',\n",
       " 'mob',\n",
       " 'plus',\n",
       " 'wen',\n",
       " 'messages',\n",
       " 'away',\n",
       " 'de',\n",
       " 'till',\n",
       " 'saw',\n",
       " 'yesterday',\n",
       " 'alright',\n",
       " 'hair',\n",
       " 'else',\n",
       " 'weekly',\n",
       " 'worry',\n",
       " 'shop',\n",
       " 'havent',\n",
       " 'bored',\n",
       " 'attempt',\n",
       " 'update',\n",
       " 'music',\n",
       " 'dude',\n",
       " 'book',\n",
       " 'oso',\n",
       " 'run',\n",
       " 'yours',\n",
       " 'making',\n",
       " 'double',\n",
       " 'goin',\n",
       " 'stay',\n",
       " 'makes',\n",
       " 'optout',\n",
       " 'colour',\n",
       " 'haf',\n",
       " 'lei',\n",
       " 'food',\n",
       " 'these',\n",
       " 'haven',\n",
       " 'online',\n",
       " 'words',\n",
       " 'hurt',\n",
       " 'id',\n",
       " 'coz',\n",
       " 'net',\n",
       " 'tried',\n",
       " 'hot',\n",
       " 'national',\n",
       " 'either',\n",
       " 'question',\n",
       " 'top',\n",
       " 'feeling',\n",
       " 'dreams',\n",
       " 'club',\n",
       " 'friendship',\n",
       " 'gr',\n",
       " 'driving',\n",
       " 'test',\n",
       " 'change',\n",
       " 'yourself',\n",
       " 'ard',\n",
       " 'address',\n",
       " 'hours',\n",
       " 'family',\n",
       " 'delivery',\n",
       " 'answer',\n",
       " 'sch',\n",
       " 'tot',\n",
       " 'order',\n",
       " 'todays',\n",
       " 'trip',\n",
       " 'beautiful',\n",
       " 'wants',\n",
       " 'movie',\n",
       " 'together',\n",
       " 'bonus',\n",
       " 'full',\n",
       " 'busy',\n",
       " 'nt',\n",
       " 'calling',\n",
       " 'game',\n",
       " 'http',\n",
       " 'believe',\n",
       " 'comes',\n",
       " 'sae',\n",
       " 'both',\n",
       " 'vouchers',\n",
       " 'date',\n",
       " 'lose',\n",
       " 'wid',\n",
       " 'sad',\n",
       " 'story',\n",
       " 'mean',\n",
       " 'games',\n",
       " 'poly',\n",
       " 'eve',\n",
       " 'saying',\n",
       " 'drive',\n",
       " 'huh',\n",
       " 'walk',\n",
       " 'old',\n",
       " 'charge',\n",
       " 'leaving',\n",
       " 'sleeping',\n",
       " 'points',\n",
       " 'ia',\n",
       " 'happen',\n",
       " 'chikku',\n",
       " 'smiling',\n",
       " 'noe',\n",
       " 'await',\n",
       " 'ring',\n",
       " 'info',\n",
       " 'set',\n",
       " 'row',\n",
       " 'suite',\n",
       " 'congrats',\n",
       " 'pounds',\n",
       " 'wil',\n",
       " 'aft',\n",
       " 'pic',\n",
       " 'email',\n",
       " 'thinking',\n",
       " 'took',\n",
       " 'tomo',\n",
       " 'pics',\n",
       " 'finished',\n",
       " 'awesome',\n",
       " 'minute',\n",
       " 'simple',\n",
       " 'rite',\n",
       " 'news',\n",
       " 'private',\n",
       " 'mum',\n",
       " 'started',\n",
       " 'brother',\n",
       " 'post',\n",
       " 'mths',\n",
       " 'hl',\n",
       " 'okie',\n",
       " 'touch',\n",
       " 'forget',\n",
       " 'drink',\n",
       " 'auction',\n",
       " 'everyone',\n",
       " 'final',\n",
       " 'tho',\n",
       " 'msgs',\n",
       " 'lets',\n",
       " 'head',\n",
       " 'second',\n",
       " 'mine',\n",
       " 'smoke',\n",
       " 'angry',\n",
       " 'unsubscribe',\n",
       " 'close',\n",
       " 'pub',\n",
       " 'anyone',\n",
       " 'neva',\n",
       " 'gd',\n",
       " 'available',\n",
       " 'drop',\n",
       " 'taking',\n",
       " 'land',\n",
       " 'services',\n",
       " 'cause',\n",
       " 'sister',\n",
       " 'loving',\n",
       " 'sis',\n",
       " 'opt',\n",
       " 'worth',\n",
       " 'found',\n",
       " 'lesson',\n",
       " 'lucky',\n",
       " 'sun',\n",
       " 'choose',\n",
       " 'lots',\n",
       " 'knw',\n",
       " 'alone',\n",
       " 'search',\n",
       " 'company',\n",
       " 'dating',\n",
       " 'open',\n",
       " 'parents',\n",
       " 'whats',\n",
       " 'each',\n",
       " 'sounds',\n",
       " 'voucher',\n",
       " 'card',\n",
       " 'carlos',\n",
       " 'break',\n",
       " 'statement',\n",
       " 'whatever',\n",
       " 'doesn',\n",
       " 'expires',\n",
       " 'ha',\n",
       " 'yr',\n",
       " 'lands',\n",
       " 'saturday',\n",
       " 'visit',\n",
       " 'decided',\n",
       " 'takes',\n",
       " 'fast',\n",
       " 'boytoy',\n",
       " 'prob',\n",
       " 'mom',\n",
       " 'gone',\n",
       " 'frnd',\n",
       " 'nyt',\n",
       " 'hard',\n",
       " 'type',\n",
       " 'anytime',\n",
       " 'smth',\n",
       " 'wonderful',\n",
       " 'gbp',\n",
       " 'far',\n",
       " 'ltd',\n",
       " 'rs',\n",
       " 'girls',\n",
       " 'identifier',\n",
       " 'bout',\n",
       " 'happened',\n",
       " 'needs',\n",
       " 'hows',\n",
       " 'treat',\n",
       " 'kind',\n",
       " 'ni',\n",
       " 'winner',\n",
       " 'mobileupd',\n",
       " 'college',\n",
       " 'fri',\n",
       " 'congratulations',\n",
       " 'hit',\n",
       " 'finally',\n",
       " 'lovely',\n",
       " 'least',\n",
       " 'used',\n",
       " 'fucking',\n",
       " 'party',\n",
       " 'light',\n",
       " 'wit',\n",
       " 'camcorder',\n",
       " 'sea',\n",
       " 'friday',\n",
       " 'read',\n",
       " 'pretty',\n",
       " 'goodmorning',\n",
       " 'mrng',\n",
       " 'mates',\n",
       " 'hand',\n",
       " 'hold',\n",
       " 'tel',\n",
       " 'quiz',\n",
       " 'operator',\n",
       " 'outside',\n",
       " 'weeks',\n",
       " 'nope',\n",
       " 'oredi',\n",
       " 'wrong',\n",
       " 'player',\n",
       " 'secret',\n",
       " 'crazy',\n",
       " 'darlin',\n",
       " 'content',\n",
       " 'chennai',\n",
       " 'unlimited',\n",
       " 'course',\n",
       " 'mu',\n",
       " 'tc',\n",
       " 'listen',\n",
       " 'frm',\n",
       " 'snow',\n",
       " 'fone',\n",
       " 'offers',\n",
       " 'cum',\n",
       " 'savamob',\n",
       " 'seeing',\n",
       " 'their',\n",
       " 'costa',\n",
       " 'credit',\n",
       " 'wkly',\n",
       " 'wq',\n",
       " 'fr',\n",
       " 'jay',\n",
       " 'meant',\n",
       " 'na',\n",
       " 'reason',\n",
       " 'mate',\n",
       " 'case',\n",
       " 'hmm',\n",
       " 'blue',\n",
       " 'ten',\n",
       " 'freemsg',\n",
       " 'thinks',\n",
       " 'log',\n",
       " 'hungry',\n",
       " 'telling',\n",
       " 'earlier',\n",
       " 'frnds',\n",
       " 'bank',\n",
       " 'whole',\n",
       " 'welcome',\n",
       " 'project',\n",
       " 'fancy',\n",
       " 'sunday',\n",
       " 'india',\n",
       " 'yrs',\n",
       " 'luck',\n",
       " 'dnt',\n",
       " 'phones',\n",
       " 'point',\n",
       " 'un',\n",
       " 'couple',\n",
       " 'eh',\n",
       " 'valued',\n",
       " 'ass',\n",
       " 'father',\n",
       " 'motorola',\n",
       " 'months',\n",
       " 'etc',\n",
       " 'ago',\n",
       " 'balance',\n",
       " 'happiness',\n",
       " 'hmmm',\n",
       " 'stupid',\n",
       " 'die',\n",
       " 'currently',\n",
       " 'understand',\n",
       " 'download',\n",
       " 'bslvyl',\n",
       " 'yar',\n",
       " 'area',\n",
       " 'txts',\n",
       " 'mr',\n",
       " 'gn',\n",
       " 'enter',\n",
       " 'talking',\n",
       " 'almost',\n",
       " 'within',\n",
       " 'march',\n",
       " 'mayb',\n",
       " 'invited',\n",
       " 'numbers',\n",
       " 'caller',\n",
       " 'reading',\n",
       " 'cut',\n",
       " 'support',\n",
       " 'lost',\n",
       " 'press',\n",
       " 'sk',\n",
       " 'knew',\n",
       " 'felt',\n",
       " 'tired',\n",
       " 'mah',\n",
       " 'mobiles',\n",
       " 'sex',\n",
       " 'gas',\n",
       " 'hee',\n",
       " 'park',\n",
       " 'christmas',\n",
       " 'side',\n",
       " 'reveal',\n",
       " 'wasn',\n",
       " 'questions',\n",
       " 'song',\n",
       " 'gym',\n",
       " 'ish',\n",
       " 'confirm',\n",
       " 'darren',\n",
       " 'information',\n",
       " 'askd',\n",
       " 'loads',\n",
       " 'correct',\n",
       " 'computer',\n",
       " 'gotta',\n",
       " 'complimentary',\n",
       " 'supposed',\n",
       " 'sending',\n",
       " 'met',\n",
       " 'ends',\n",
       " 'joy',\n",
       " 'ac',\n",
       " 'picking',\n",
       " 'charged',\n",
       " 'red',\n",
       " 'grins',\n",
       " 'doin',\n",
       " 'pass',\n",
       " 'uncle',\n",
       " 'wow',\n",
       " 'swing',\n",
       " 'redeemed',\n",
       " 'ge',\n",
       " 'bcoz',\n",
       " 'ugh',\n",
       " 'rental',\n",
       " 'via',\n",
       " 'gal',\n",
       " 'difficult',\n",
       " 'surprise',\n",
       " 'comp',\n",
       " 'john',\n",
       " 'xy',\n",
       " 'ans',\n",
       " 'semester',\n",
       " 'ipod',\n",
       " 'laptop',\n",
       " 'through',\n",
       " 'heard',\n",
       " 'direct',\n",
       " 'extra',\n",
       " 'seen',\n",
       " 'wonder',\n",
       " 'feels',\n",
       " 'orchard',\n",
       " 'safe',\n",
       " 'truth',\n",
       " 'wap',\n",
       " 'store',\n",
       " 'sell',\n",
       " 'paper',\n",
       " 'entered',\n",
       " 'kate',\n",
       " 'dream',\n",
       " 'loved',\n",
       " 'slowly',\n",
       " 'dona',\n",
       " 'whenever',\n",
       " 'isn',\n",
       " 'sound',\n",
       " 'eg',\n",
       " 'sort',\n",
       " 'shower',\n",
       " 'bath',\n",
       " 'wine',\n",
       " 'asap',\n",
       " 'ts',\n",
       " 'muz',\n",
       " 'dogging',\n",
       " 'wana',\n",
       " 'asking',\n",
       " 'blood',\n",
       " 'reward',\n",
       " 'comin',\n",
       " 'weed',\n",
       " 'lovable',\n",
       " 'rply',\n",
       " 'hmv',\n",
       " 'small',\n",
       " 'abiola',\n",
       " 'crave',\n",
       " 'gets',\n",
       " 'ex',\n",
       " 'usf',\n",
       " 'somebody',\n",
       " 'exam',\n",
       " 'remove',\n",
       " 'rest',\n",
       " 'plans',\n",
       " 'pete',\n",
       " 'terms',\n",
       " 'std',\n",
       " 'police',\n",
       " 'own',\n",
       " 'monday',\n",
       " 'checking',\n",
       " 'nobody',\n",
       " 'hg',\n",
       " 'yep',\n",
       " 'txting',\n",
       " 'noon',\n",
       " 'match',\n",
       " 'clean',\n",
       " 'discount',\n",
       " 'slow',\n",
       " 'rock',\n",
       " 'move',\n",
       " 'otherwise',\n",
       " 'voice',\n",
       " 'rates',\n",
       " 'immediately',\n",
       " 'fantastic',\n",
       " 'different',\n",
       " 'street',\n",
       " 'nah',\n",
       " 'callertune',\n",
       " 'ntt',\n",
       " 'cr',\n",
       " 'reached',\n",
       " 'hospital',\n",
       " 'mm',\n",
       " 'gave',\n",
       " 'getzed',\n",
       " 'forever',\n",
       " 'link',\n",
       " 'worried',\n",
       " 'booked',\n",
       " 'rakhesh',\n",
       " 'moment',\n",
       " 'sony',\n",
       " 'energy',\n",
       " 'poor',\n",
       " 'ldn',\n",
       " 'wc',\n",
       " 'woke',\n",
       " 'water',\n",
       " 'wishing',\n",
       " 'custcare',\n",
       " 'glad',\n",
       " 'bathe',\n",
       " 'myself',\n",
       " 'gettin',\n",
       " 'knows',\n",
       " 'figure',\n",
       " 'england',\n",
       " 'convey',\n",
       " 'admirer',\n",
       " 'empty',\n",
       " 'merry',\n",
       " 'king',\n",
       " 'il',\n",
       " 'usual',\n",
       " 'film',\n",
       " 'em',\n",
       " 'drugs',\n",
       " 'kids',\n",
       " ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the indexes of the word2vec model.\n",
    "w2v_model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another word2vec test to find similar words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('creepy', 0.9978950619697571),\n",
       " ('traffic', 0.9978407025337219),\n",
       " ('wipro', 0.9975351691246033),\n",
       " ('admit', 0.997498631477356),\n",
       " ('uh', 0.9973968863487244)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('film',topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the index from a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.key_to_index[\"film\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction function for point 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5572, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This function creates the feature vector for each sentence. \n",
    "#Based on https://www.kaggle.com/code/wolfgangb33r/toxic-wikipedia-comment-word2vec-mlpclassifier\n",
    "\n",
    "def createFeatureVector(words, model, max_length_words, num_features):\n",
    "        # Pre-initialize an empty numpy array (for speed)\n",
    "        featureVec = np.zeros((num_features,), dtype=\"float32\")\n",
    "        #\n",
    "        nwords = 0\n",
    "        # \n",
    "        # Index2word is a list that contains the names of the words in \n",
    "        # the model's vocabulary. Convert it to a set, for speed \n",
    "        #index2word_set = set(model.wv.index2word)\n",
    "        index2word_set = set(model.wv.index_to_key)\n",
    "        #\n",
    "        # Loop over each word in the review and, if it is in the model's\n",
    "        # vocaublary, add its feature vector to the total\n",
    "        for word in words:\n",
    "            if len(word) <= max_length_words:\n",
    "\n",
    "                if word in index2word_set: \n",
    "                    nwords = nwords + 1\n",
    "                    featureVec = np.add(featureVec, w2v_model.wv.key_to_index[word])\n",
    "                    #featureVec = np.add(featureVec, model.wv.get_item())\n",
    "        # Divide the result by the number of words to get the average\n",
    "        if nwords == 0:\n",
    "            nwords = 1\n",
    "        featureVec = np.divide(featureVec, nwords)\n",
    "        return featureVec\n",
    "\n",
    "def extract_features_dataset(model, preprocessed_dataset, max_length_words, num_features):\n",
    "        # Given a set of reviews (each one a list of words), calculate \n",
    "        # the average feature vector for each one and return a 2D numpy array \n",
    "        # Preallocate a 2D numpy array, for speed\n",
    "        reviewFeatureVecs = np.zeros((len(preprocessed_dataset), num_features), dtype=\"float32\")\n",
    "        counter = 0\n",
    "        # Loop through the reviews\n",
    "        for review in preprocessed_dataset:\n",
    "            # Call the function (defined above) that makes average feature vectors\n",
    "            reviewFeatureVecs[counter] = createFeatureVector(review, model, max_length_words, num_features)\n",
    "            counter = counter + 1\n",
    "        \n",
    "        reviewFeatureVecs = torch.from_numpy(reviewFeatureVecs)\n",
    "        return reviewFeatureVecs\n",
    "    \n",
    "    \n",
    "test = extract_features_dataset(w2v_model, corpus,100,10)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 MLP implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator\n",
    "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches.\n",
    "\n",
    "Taken from the provided \"Natural_disaster_NLP_LSTM.ipynb\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''  \n",
    "\tdef __init__(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tInits the dataset mapper\n",
    "\t\t\"\"\"\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the length of the dataset\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\"\"\"\n",
    "\t\tFetches a specific item by id\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.x[idx], self.y[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load training data\n",
    "\n",
    "Taken from the provided \"Natural_disaster_NLP_LSTM.ipynb\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(batch_size = 64):\n",
    "\n",
    "  #create data loaders\n",
    "  training_set = DatasetMaper(xtrain, ytrain)\n",
    "  test_set = DatasetMaper(xtest,ytest)\n",
    "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "  loader_test = DataLoader(test_set, batch_size=batch_size)\n",
    "  \n",
    "  return loader_training, loader_test\n",
    "\n",
    "\n",
    "loader_training, loader_test = create_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=6307, out_features=500, bias=True)\n",
      "  (1): Tanh()\n",
      "  (2): Linear(in_features=500, out_features=250, bias=True)\n",
      "  (3): Tanh()\n",
      "  (4): Linear(in_features=250, out_features=2, bias=True)\n",
      "  (5): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def create_MLP_model():\n",
    "    # Model creation with neural net Sequential model\n",
    "    model=nn.Sequential(nn.Linear(6307,500), # 1 layer:- 784 input 128 o/p\n",
    "                        nn.Tanh(),\n",
    "                        #nn.ReLU(),          # Defining Regular linear unit as activation\n",
    "                        #nn.Sigmoid(),\n",
    "                        nn.Linear(500,250),  # 2 Layer:- 128 Input and 64 O/p\n",
    "                        nn.Tanh(),          # Defining Regular linear unit as activation\n",
    "                        #nn.Sigmoid(),\n",
    "                        #nn.ReLU(),\n",
    "                        nn.Linear(250,2),   # 3 Layer:- 64 Input and 10 O/P as (0-9)\n",
    "                        nn.LogSoftmax(dim=1) # Defining the log softmax to find the probablities for the last output unit\n",
    "                      ) \n",
    "    return model\n",
    "\n",
    "mlp_model = create_MLP_model()\n",
    "\n",
    "#error function\n",
    "criterion = nn.NLLLoss() \n",
    "print(\"MLP model\")\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training MLP model\n",
      "- Epoch 0 - Training loss: 0.2244411890860647\n",
      "- Epoch 1 - Training loss: 0.03784744873077476\n",
      "- Epoch 2 - Training loss: 0.019629483157768846\n",
      "- Epoch 3 - Training loss: 0.011116463002066927\n",
      "- Epoch 4 - Training loss: 0.007239337434293702\n",
      "- Epoch 5 - Training loss: 0.0034544967420515604\n",
      "- Epoch 6 - Training loss: 0.0018201478212306807\n",
      "- Epoch 7 - Training loss: 0.0009951960694577014\n",
      "- Epoch 8 - Training loss: 0.0006640992933950787\n",
      "- Epoch 9 - Training loss: 0.0005027468086284768\n",
      "\n",
      "Training Time (in minutes) = 0.17952746550242107\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, epochs = 15, lr = 0.01, is_MLP = False):\n",
    "\n",
    "    time0 = time()    \n",
    "    running_loss_list= []\n",
    "    epochs_list = []\n",
    "    optimizer = optim.SGD(model.parameters(), lr= lr, momentum=0.9)\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        \n",
    "        #go for every batch\n",
    "        for x_batch, y_batch in loader_training:\n",
    "            \n",
    "            x = x_batch.type(torch.FloatTensor)\n",
    "            y = y_batch.type(torch.LongTensor)\n",
    "            \n",
    "            # Flatenning\n",
    "            if(is_MLP):\n",
    "              x = x.view(x.shape[0], -1) \n",
    "            \n",
    "            # defining gradient in each epoch as 0\n",
    "            optimizer.zero_grad()            \n",
    "            # modeling for each image batch\n",
    "            output = model(x)\n",
    "\n",
    "            # calculating the loss\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            # And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculating the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            print(\"- Epoch {} - Training loss: {}\".format(e, running_loss/len(loader_training)))\n",
    "            \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "    return model\n",
    "\n",
    "print(\"### Training MLP model\")\n",
    "mlp_model = train_model(mlp_model, criterion, epochs = 10, lr = 0.1, is_MLP = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MLP model\n",
      "Messages Tested = 1115\n",
      "Correct Tests = 1099\n",
      "False Positive Tests = 2\n",
      "False Negative Tests = 14\n",
      "\n",
      "Model Accuracy (Average) = 98.565 %\n"
     ]
    }
   ],
   "source": [
    "def test_model(testloader, model, verbose = True):\n",
    "    correct_rate, false_negative_rate, all_count = 0, 0, 0\n",
    "    \n",
    "    for x_batch, y_batch in loader_test:\n",
    "\n",
    "      x = x_batch.type(torch.FloatTensor)\n",
    "      y = y_batch.type(torch.LongTensor)\n",
    "\n",
    "      for i in range(len(y)): # se itera sobre los índices de targets\n",
    "        text = x[i].view(1, 6307)\n",
    "        with torch.no_grad():\n",
    "            logps = model(text)\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.cpu().numpy()[0])\n",
    "        pred_label = probab.index(max(probab)) # Se obtiene el target predicho de la iteración actual\n",
    "        true_label = y_batch.cpu().numpy()[i] # Se obtiene el target correcto de la iteración actual\n",
    "        \n",
    "        if (true_label == pred_label): correct_rate += 1 # Predicción correcta si igual a target\n",
    "        else:\n",
    "          if (pred_label == 0): false_negative_rate += 1 # Falso negativo si predicción es 0 pero correcto 1\n",
    "\n",
    "        all_count += 1\n",
    "\n",
    "    if (verbose):\n",
    "      print(\"Messages Tested =\", all_count)\n",
    "      print(\"Correct Tests =\", correct_rate)\n",
    "      print(\"False Positive Tests =\", (all_count - correct_rate) - false_negative_rate)\n",
    "      print(\"False Negative Tests =\", false_negative_rate)\n",
    "      print(\"\\nModel Accuracy (Average) =\", np.round((correct_rate/all_count)*100,4),\"%\")\n",
    "\n",
    "    return correct_rate, false_negative_rate, all_count\n",
    "\n",
    "print(\"Testing MLP model\")\n",
    "res = test_model(loader_training, mlp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5411a088105c78e15883c58bf538f4089f9dad56c8db4bd0204f934052f24494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
