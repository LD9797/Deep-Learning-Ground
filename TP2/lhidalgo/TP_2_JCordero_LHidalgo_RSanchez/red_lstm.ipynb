{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Redes Recurrentes y Representaciones Incrustadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. (100 puntos) Red LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De ser necesario ejecutar lo siguiente para descargar paquetes de NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 200 # Establece el TOP de palabras que se utilizan en el diccionario de palabras, el tamaño de la representación incrustada.\n",
    "BATCH_SIZE = 64 # Tamaño del batch que se utilizará para el dataset de entrenamiento.\n",
    "COLLECTION_PATH = \".\\\\smsspamcollection\" # Ruta base para encontrar el dataset de pruebas de spam.\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones requeridas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código Provisto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corresponde con el código encontrado en el archivo \"Natural_disaster_NLP_LSTM.ipynb\", del cual se realizaron las modificaciones y consideraciones que fueron mencionadas durante las clases del curso.\n",
    "\n",
    "#### Dataset mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMaper(Dataset):\n",
    "    '''\n",
    "    Handles batches of dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Inits the dataset mapper\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a specific item by id\n",
    "        \"\"\"\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Modelo de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TweetClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, batch_size=BATCH_SIZE, hidden_dim=20, lstm_layers=2, max_words=MAX_WORDS):\n",
    "        \"\"\"\n",
    "        param batch_size: batch size for training data\n",
    "        param hidden_dim: number of hidden units used in the LSTM and the Embedding layer\n",
    "        param lstm_layers: number of lstm_layers\n",
    "        param max_words: maximum sentence length\n",
    "        \"\"\"\n",
    "        super(LSTM_TweetClassifier, self).__init__()\n",
    "        # batch size during training\n",
    "        self.batch_size = batch_size\n",
    "        # number of hidden units in the LSTM layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of LSTM layers\n",
    "        self.LSTM_layers = lstm_layers\n",
    "        self.input_size = max_words  # embedding dimension\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)  # Para descartar\n",
    "        #  N, D\t\t\t#  hidden_dim -> To set embedding size\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)  # learn representation\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers,\n",
    "                            batch_first=True)  \n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
    "        self.fc2 = nn.Linear(257, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        param x: model input\n",
    "        \"\"\"\n",
    "        # it starts with noisy estimations of h and c\n",
    "        h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  # \"Context\"\n",
    "        c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  # \"State\"\n",
    "        # Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution.\n",
    "        # The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std)\n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(out, (h, c))\n",
    "        out = self.dropout(out)\n",
    "        #  Fully connected network para la clasificacion\n",
    "        out = torch.relu_(self.fc1(out[:, -1, :]))\n",
    "        out = self.dropout(out)\n",
    "        # sigmoid activation function\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Función de entrenamiento y evaluación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuray(y_pred, y_gt):\n",
    "    return accuracy_score(y_pred, y_gt)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader_test):\n",
    "    predictions = []\n",
    "    accuracies = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in loader_test:\n",
    "            x_batch = torch.t(torch.stack(x_batch))\n",
    "            x = x_batch.type(torch.LongTensor)\n",
    "            y = y_batch.type(torch.FloatTensor)\n",
    "            y_pred = model(x)\n",
    "            y_pred = torch.round(y_pred).flatten()\n",
    "            predictions += list(y_pred.detach().numpy())\n",
    "            acc_batch = accuracy_score(y_pred, y)\n",
    "            accuracies += [acc_batch]\n",
    "    return np.array(accuracies)\n",
    "\n",
    "\n",
    "def train_model(model, epochs=EPOCHS, learning_rate=0.01):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        loss_dataset = 0\n",
    "        for x_batch, y_batch in loader_training:\n",
    "            x_batch = torch.t(torch.stack(x_batch))\n",
    "            x = x_batch.type(torch.LongTensor)\n",
    "            y = y_batch.type(torch.FloatTensor)\n",
    "            y_pred = model(x).flatten()\n",
    "            loss = F.binary_cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            loss_dataset += loss\n",
    "        accuracies = evaluate_model(model, loader_test)\n",
    "        print(\"Epoch #\", epoch, \" Loss training: \", loss_dataset.item(), \" Accuracy test (mean): \", accuracies.mean(), \" Standard deviation: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código base implementado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Preprocesamientos\n",
    "\n",
    "Métodos de preprocesado implementados según indicaciones del punto 1.a del TP2. Esto incluye la implementación de los métodos *preprocesar_documento_1*, *preprocesar_documento_2* y *preprocess_example*. Este último tiene por objetivo presentar la ejecución de los dos primeros métodos citados en la sección de *Resolución de ejercicios* de más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_documento_1(document, to_string=False):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(document)\n",
    "    word_tokens = [token.lower() for token in word_tokens]\n",
    "    word_tokens = [token for token in word_tokens if token.isalnum()]  # To remove punctuations\n",
    "    filtration = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtration if not to_string else ' '.join(filtration)\n",
    "\n",
    "def preprocesar_documento_2(document, to_string=False):\n",
    "    word_tokens = preprocesar_documento_1(document)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtration = [lemmatizer.lemmatize(word, pos=\"v\") for word in word_tokens]\n",
    "    return filtration if not to_string else ' '.join(filtration)\n",
    "\n",
    "def preprocess_example():\n",
    "    with open(COLLECTION_PATH + \"\\\\SMSSpamCollection\", 'r') as collection:\n",
    "        for line in collection:\n",
    "            d0 = line.replace(\"ham\", \"\").replace(\"spam\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            d1 = \"I thought, I thought of thinking of thanking you for the gift\"\n",
    "            d2 = \"She was thinking of going to go and get you a GIFT!\"\n",
    "            print(\"Testing preprocessing methods with different lines:\\n\\n{}\\n{}\\n{}\".format(print_test(1, d0), print_test(2, d1), print_test(3, d2)))\n",
    "            break\n",
    "\n",
    "def print_test(test, line):\n",
    "    return \"* Test line #{}: {}\\nPreprocess #1: {}\\nPreprocess #2: {}\\n\".format(test, line, preprocesar_documento_1(line, to_string=True), preprocesar_documento_2(line, to_string=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Tokenizer\n",
    "\n",
    "Los métodos *tokens_to_indexes*, *sequence_to_number_combination* y *adapt_to_input_layer* son implementaciones generadas como \"equivalentes\" para las funciones implementadas en el código provisto llamadas *prepare_tokens* y *sequence_to_token*. Su objetivo principal es, en tres pasos bien definidos por método:\n",
    "\n",
    "1. Establecer el *diccionario* del TOP *MAX_WORDS* (cuya longitud se encuentra en la sección de *Constantes*) para la posterior definición de representación numérica para las líneas preprocesadas.\n",
    "2. Generar la representación equivalente de cada línea según los valores numéricos obtenidos del diccionario creado en el punto anterior (se ignoran aquellas palabras que no estén en el diccionario).\n",
    "3. Adaptar la representación obtenida del punto anterior al tamaño *MAX_WORDS* para que coincida con la entrada para el entrenamiento de este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indexes(sentences: list) -> dict:\n",
    "    words_count = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in words_count:\n",
    "                words_count[word] += 1\n",
    "            else:\n",
    "                words_count[word] = 1\n",
    "    words_to_list = list(dict(sorted(words_count.items(), key=lambda item: item[1])))\n",
    "    words_to_list.reverse()\n",
    "    top_max_words = words_to_list[0: MAX_WORDS - 1]\n",
    "    index_words = {}\n",
    "    counter = 1\n",
    "    for word in top_max_words:\n",
    "        index_words[word] = counter\n",
    "        counter += 1\n",
    "    return index_words\n",
    "\n",
    "def sequence_to_number_combination(word: list, index_words: dict):\n",
    "    sequence = []\n",
    "    for token in word:\n",
    "        if token in index_words:\n",
    "            sequence.append(index_words[token])\n",
    "    return sequence\n",
    "\n",
    "def adapt_to_input_layer(dataset, input_layer_size=MAX_WORDS):\n",
    "    new_dataset = []\n",
    "    for data in dataset:\n",
    "        zeros_to_add = input_layer_size - len(data)\n",
    "        new_data_list = [0 for zero in range(zeros_to_add)]\n",
    "        new_data_list.extend(data)\n",
    "        new_dataset.append(new_data_list)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data Loader\n",
    "\n",
    "Este método concentra todo lo discutido anteriormente y termina por generar los datasets de entrenamiento y pruebas por utilizar en la siguiente sección de *Resolución de ejercicios*, más específicamente en los puntos 1.b y 1.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_data(f_prepros, test_size=0.4, train_size=0.6):\n",
    "    # Load dataset\n",
    "    dataset_frame = pd.read_csv(COLLECTION_PATH + '\\\\SMSSpamCollection', delimiter='\\t', header=None)\n",
    "    # Preprocess document\n",
    "    sentences_list = [f_prepros(sentence) for sentence in dataset_frame[1]]\n",
    "    # Add index to every word\n",
    "    words_dictionary = tokens_to_indexes(sentences_list)\n",
    "    # Transform tokens (words) to indexes (numbers)\n",
    "    sentences_list = [sequence_to_number_combination(sentence, words_dictionary) for sentence in sentences_list]\n",
    "    # One-hot tags ham = 0, spam = 1\n",
    "    tags_list = [0 if tag == \"ham\" else 1 for tag in dataset_frame[0]]\n",
    "    # Build train and test datasets\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(sentences_list, tags_list, test_size=test_size, train_size=train_size)\n",
    "    # Adapt data to input layer\n",
    "    x_train = adapt_to_input_layer(X_train_raw)\n",
    "    x_test = adapt_to_input_layer(X_test_raw)\n",
    "    training_set = DatasetMaper(x_train, y_train)\n",
    "    test_set = DatasetMaper(x_test, y_test)\n",
    "    loader_training = DataLoader(training_set, batch_size=BATCH_SIZE)\n",
    "    loader_test = DataLoader(test_set)\n",
    "    return loader_training, loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolución de ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, haciendo uso del código provisto e implementado anteriormente, se dará resolución a los ejercicios propuestos en el punto #1 del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implemente la siguiente arquitectura de una red LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. 1) Muestre un ejemplo con una entrada escogida del pre-procesamiento con ambos enfoques, y explique brevemente los posibles efectos de utilizar el segundo enfoque al primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing methods with different lines:\n",
      "\n",
      "* Test line #1: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Preprocess #1: go jurong point crazy available bugis n great world la e buffet cine got amore wat\n",
      "Preprocess #2: go jurong point crazy available bugis n great world la e buffet cine get amore wat\n",
      "\n",
      "* Test line #2: I thought, I thought of thinking of thanking you for the gift\n",
      "Preprocess #1: thought thought thinking thanking gift\n",
      "Preprocess #2: think think think thank gift\n",
      "\n",
      "* Test line #3: She was thinking of going to go and get you a GIFT!\n",
      "Preprocess #1: thinking going go get gift\n",
      "Preprocess #2: think go go get gift\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R/** En el primer preprocesamiento se emplea únicamente la eliminación de mayúsculas, signos de puntuación y *stopwords*. Estas últimas son palabras cuya significancia es mínima en el procesamiento de lenguaje natural por lo que es posible retirarlas sin generar mayor impacto en el entendimiento de las expresiones. Como se puede observar en las anteriores entradas, su funcionamiento consiste en estandarizar la expresión al retirar mayúsculas y puntuación, además de eliminar preposiciones y palabras que sean entendidas como *stopwords* por *nltk*. Puede observarse en el *test line #1* como la palabra *until* desaparece al ser preprocesada. Entre los ejemplos que se muestran, el *test line #1* elimina palabras como *until*, *only* e *in*, mientras que en el *test line #2* también se retiran *of*, *you*, *for* y *the*. Esto evidentemente reduce el espectro de palabras a tomar en cuenta para el procesamiento.\n",
    "\n",
    "En el segundo preprocesamiento, aparte de aplicar primero lo expuesto anteriormente, se emplea también la *lematización* de la expresión. Esto permite obtener la *raíz* de una palabra a partir de cualquiera de sus variables posibles, tal como se puede observar en los ejemplos anteriores donde *thinking* pasa a su forma base *think*. Además de eso, otras funciones con las que cuenta es reducir las palabras a su forma singular, cambiar el tiempo, entre otros. Por ejemplo, en el *test line #1* se cambia el verbo en pasado *got* por el verbo *get*, en el *test line #2* se cambian las palabras *thought*, *thinking* y *thanking* por *think*, *think* y *thank* y, finalmente, en el *test line #3* se cambian las palabras *thinking* y *going* por *think* y *go*.\n",
    "\n",
    "Los efectos individuales que se identifican para el primer preprocesamiento consiste en estandarizar todas las frases del dataset por utilizar. Esto igualmente mantiene palabras que son iguales pero que pueden estar conjugadas, haciendo que al ser procesadas se interpreten como palabras distintas, afectando el entrenamiento y posterior prueba utilizando frases muy similares pero que estén conjugadas en otros tiempos, solo por dar un ejemplo.\n",
    "\n",
    "Esto precisamente es lo que se busca con la especialización del método #1, es decir, el segundo preprocesamiento. Este no solo aprovecha las virtudes de estandarización del primer preprocesamiento, sino que también incluye la lematización, lo cual reduce aún más la gama de palabras a utilizar para el análisis, produciendo que tanto al entrenar como probar se encuentre que las palabras *get* y *got*, aunque conjugadas en distinto tiempo, corresponden a la misma palabra y tienen un significado y aplicación relativamente similares. El efecto que se identifica es que optimizará la ejecución del entrenamiento del modelo y favorecerá una mayor efectividad al momento de predecir ya que no tomará las conjugaciones y otras variantes de las palabras como palabras completamente distintas, haciendo que ahora se les pueda dar más sentido al interpretar que son la misma palabra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) 10 corridas para D=20, D=100 utilizando el segundo enfoque de preprocesamiento, resultados en tablas con medias y desviaciones estandar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss training:  16.875024795532227  Accuracy test (mean):  0.9537909376401974  Standard deviation:  0.20993757385382558\n",
      "Epoch # 1  Loss training:  6.4264678955078125  Accuracy test (mean):  0.9591745177209511  Standard deviation:  0.19788573034893636\n",
      "Epoch # 2  Loss training:  4.92019510269165  Accuracy test (mean):  0.9591745177209511  Standard deviation:  0.19788573034893636\n",
      "Epoch # 3  Loss training:  3.6861252784729004  Accuracy test (mean):  0.9569313593539704  Standard deviation:  0.20301165690406248\n",
      "Epoch # 4  Loss training:  3.0383758544921875  Accuracy test (mean):  0.9614176760879318  Standard deviation:  0.19259732135627547\n",
      "Epoch # 5  Loss training:  2.3465917110443115  Accuracy test (mean):  0.9573799910273665  Standard deviation:  0.20199887080824516\n",
      "Epoch # 6  Loss training:  2.394618272781372  Accuracy test (mean):  0.9641094661283086  Standard deviation:  0.1860172127790768\n",
      "Epoch # 7  Loss training:  1.7769286632537842  Accuracy test (mean):  0.9659039928218932  Standard deviation:  0.18147580960727866\n",
      "Epoch # 8  Loss training:  1.9297988414764404  Accuracy test (mean):  0.9627635711081203  Standard deviation:  0.18934063815055593\n",
      "Epoch # 9  Loss training:  1.6380583047866821  Accuracy test (mean):  0.9632122027815164  Standard deviation:  0.1882404185989166\n",
      "D = 20\n",
      "Final average accuracy (mean) :  0.9632122027815164 Final standard deviation:  0.1882404185989166\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and dataset selection based on the train_test_split method using 40% for test size and 60% for train size.\n",
    "# train_test_split returns random partitions for training and testing.\n",
    "loader_training, loader_test = load_process_data(preprocesar_documento_2, test_size=0.4, train_size=0.6)\n",
    "model = LSTM_TweetClassifier(hidden_dim=20)\n",
    "train_model(model, EPOCHS)\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"D = 20\")\n",
    "print(\"Final average accuracy (mean): \", accuracies.mean(), \"Final standard deviation: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss training:  68.91210174560547  Accuracy test (mean):  0.9385374607447285  Standard deviation:  0.24017680055235485\n",
      "Epoch # 1  Loss training:  7.869880676269531  Accuracy test (mean):  0.9519964109466128  Standard deviation:  0.21377381619688762\n",
      "Epoch # 2  Loss training:  4.8323163986206055  Accuracy test (mean):  0.9676985195154778  Standard deviation:  0.1767995894877311\n",
      "Epoch # 3  Loss training:  3.4339821338653564  Accuracy test (mean):  0.9672498878420817  Standard deviation:  0.1779818595013608\n",
      "Epoch # 4  Loss training:  2.911142349243164  Accuracy test (mean):  0.9726334679228353  Standard deviation:  0.16314902696379824\n",
      "Epoch # 5  Loss training:  2.7866263389587402  Accuracy test (mean):  0.9668012561686855  Standard deviation:  0.1791552043322695\n",
      "Epoch # 6  Loss training:  2.3446109294891357  Accuracy test (mean):  0.9690444145356663  Standard deviation:  0.17319739372431128\n",
      "Epoch # 7  Loss training:  2.3271267414093018  Accuracy test (mean):  0.9690444145356663  Standard deviation:  0.17319739372431128\n",
      "Epoch # 8  Loss training:  2.4226813316345215  Accuracy test (mean):  0.9685957828622701  Standard deviation:  0.17440754652163487\n",
      "Epoch # 9  Loss training:  2.00620698928833  Accuracy test (mean):  0.9699416778824586  Standard deviation:  0.17074782396569396\n",
      "D = 100\n",
      "Final average accuracy (mean):  0.9699416778824586 Final standard deviation:  0.17074782396569396\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and dataset selection based on the train_test_split method using 40% for test size and 80% for train size.\n",
    "# train_test_split returns random partitions for training and testing.\n",
    "loader_training, loader_test = load_process_data(preprocesar_documento_2, test_size=0.4, train_size=0.6)\n",
    "model = LSTM_TweetClassifier(hidden_dim=100)\n",
    "train_model(model, EPOCHS)\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"D = 100\")\n",
    "print(\"Final average accuracy (mean): \", accuracies.mean(), \"Final standard deviation: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla de resumen:\n",
    "\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>~</td>\n",
    "      <td></td>\n",
    "      <td>D = 20</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>D = 100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Epoc</td>\n",
    "      <td></td>\n",
    "      <td>Accuracy</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>Accuracy</td>\n",
    "      <td></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td>Loss</td>\n",
    "      <td>Mean</td>\n",
    "      <td>Std</td>\n",
    "      <td>Loss</td>\n",
    "      <td>Mean</td>\n",
    "      <td>Std</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>16,8750248</td>\n",
    "      <td>0,953790938</td>\n",
    "      <td>0,209937574</td>\n",
    "      <td>68,91210175</td>\n",
    "      <td>0,938537461</td>\n",
    "      <td>0,240176801</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>6,426467896</td>\n",
    "      <td>0,959174518</td>\n",
    "      <td>0,19788573</td>\n",
    "      <td>7,869880676</td>\n",
    "      <td>0,951996411</td>\n",
    "      <td>0,213773816</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>4,920195103</td>\n",
    "      <td>0,959174518</td>\n",
    "      <td>0,19788573</td>\n",
    "      <td>4,832316399</td>\n",
    "      <td>0,96769852</td>\n",
    "      <td>0,176799589</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>3,686125278</td>\n",
    "      <td>0,956931359</td>\n",
    "      <td>0,203011657</td>\n",
    "      <td>3,433982134</td>\n",
    "      <td>0,967249888</td>\n",
    "      <td>0,17798186</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>3,038375854</td>\n",
    "      <td>0,961417676</td>\n",
    "      <td>0,192597321</td>\n",
    "      <td>2,911142349</td>\n",
    "      <td>0,972633468</td>\n",
    "      <td>0,163149027</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>6</td>\n",
    "      <td>2,346591711</td>\n",
    "      <td>0,957379991</td>\n",
    "      <td>0,201998871</td>\n",
    "      <td>2,786626339</td>\n",
    "      <td>0,966801256</td>\n",
    "      <td>0,179155204</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7</td>\n",
    "      <td>2,394618273</td>\n",
    "      <td>0,964109466</td>\n",
    "      <td>0,186017213</td>\n",
    "      <td>2,344610929</td>\n",
    "      <td>0,969044415</td>\n",
    "      <td>0,173197394</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>8</td>\n",
    "      <td>1,776928663</td>\n",
    "      <td>0,965903993</td>\n",
    "      <td>0,18147581</td>\n",
    "      <td>2,327126741</td>\n",
    "      <td>0,969044415</td>\n",
    "      <td>0,173197394</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>9</td>\n",
    "      <td>1,929798841</td>\n",
    "      <td>0,962763571</td>\n",
    "      <td>0,189340638</td>\n",
    "      <td>2,422681332</td>\n",
    "      <td>0,968595783</td>\n",
    "      <td>0,174407547</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>10</td>\n",
    "      <td>1,638058305</td>\n",
    "      <td>0,963212203</td>\n",
    "      <td>0,188240419</td>\n",
    "      <td>2,006206989</td>\n",
    "      <td>0,969941678</td>\n",
    "      <td>0,170747824</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mean</td>\n",
    "      <td></td>\n",
    "      <td>0,963212203</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>0,969941678</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Std</td>\n",
    "      <td></td>\n",
    "      <td>0,188240419</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>0,170747824</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, cuando $D=20$, el loss y precisión inicial son menores comparado con $D=100$; al aumentar la dimensionalidad de la capa de embedding, se aumenta el vector de características, esto a largo plazo puede facilitar la detección de patrones más extensos pero afecta el proceso de entrenamiento en velocidad y cantidad de épocas necesarias para llegar a ese punto. Al final de las ejecuciones, sin embargo, en ambos lados se puede ver una precisión muy similar en media $0.9632$ vs $0.9699$ y desviación estándar $0.1882$ vs $0.1707$, aunque para $D=100$, el loss no disminuyó tanto, se mantuvo superior al inicio y al final. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) 10 corridas para D=20, D=100 utilizando el primer enfoque de preprocesamiento, resultados en tablas con medias y desviaciones estandar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss training:  15.560389518737793  Accuracy test (mean):  0.9560340960071781  Standard deviation:  0.20501927538384254\n",
      "Epoch # 1  Loss training:  7.25687837600708  Accuracy test (mean):  0.9632122027815164  Standard deviation:  0.18824041859891663\n",
      "Epoch # 2  Loss training:  5.014931678771973  Accuracy test (mean):  0.9623149394347241  Standard deviation:  0.19043344447724353\n",
      "Epoch # 3  Loss training:  4.117300033569336  Accuracy test (mean):  0.9663526244952894  Standard deviation:  0.18031979820961322\n",
      "Epoch # 4  Loss training:  3.5236101150512695  Accuracy test (mean):  0.9654553611484971  Standard deviation:  0.18262340150737022\n",
      "Epoch # 5  Loss training:  2.5243136882781982  Accuracy test (mean):  0.9663526244952894  Standard deviation:  0.18031979820961322\n",
      "Epoch # 6  Loss training:  2.147249460220337  Accuracy test (mean):  0.9650067294751009  Standard deviation:  0.18376273164836845\n",
      "Epoch # 7  Loss training:  2.262073040008545  Accuracy test (mean):  0.9645580978017048  Standard deviation:  0.18489395275903883\n",
      "Epoch # 8  Loss training:  2.221160650253296  Accuracy test (mean):  0.9663526244952894  Standard deviation:  0.18031979820961322\n",
      "Epoch # 9  Loss training:  1.4608172178268433  Accuracy test (mean):  0.9659039928218932  Standard deviation:  0.1814758096072787\n",
      "D = 20\n",
      "Final average accuracy (mean):  0.9659039928218932 Final standard deviation:  0.1814758096072787\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and dataset selection based on the train_test_split method using 40% for test size and 60% for train size.\n",
    "# train_test_split returns random partitions for training and testing.\n",
    "loader_training, loader_test = load_process_data(preprocesar_documento_1, test_size=0.4, train_size=0.6)\n",
    "model = LSTM_TweetClassifier(hidden_dim=20)\n",
    "train_model(model, EPOCHS)\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"D = 20\")\n",
    "print(\"Final average accuracy (mean): \", accuracies.mean(), \"Final standard deviation: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### D=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0  Loss training:  33.43926239013672  Accuracy test (mean):  0.946164199192463  Standard deviation:  0.2256933923688246\n",
      "Epoch # 1  Loss training:  8.200517654418945  Accuracy test (mean):  0.9605204127411395  Standard deviation:  0.19473302095107162\n",
      "Epoch # 2  Loss training:  5.922861576080322  Accuracy test (mean):  0.9641094661283086  Standard deviation:  0.1860172127790768\n",
      "Epoch # 3  Loss training:  4.44744873046875  Accuracy test (mean):  0.9632122027815164  Standard deviation:  0.18824041859891663\n",
      "Epoch # 4  Loss training:  3.727796792984009  Accuracy test (mean):  0.9650067294751009  Standard deviation:  0.18376273164836845\n",
      "Epoch # 5  Loss training:  3.3696773052215576  Accuracy test (mean):  0.9560340960071781  Standard deviation:  0.20501927538384251\n",
      "Epoch # 6  Loss training:  3.1202781200408936  Accuracy test (mean):  0.9623149394347241  Standard deviation:  0.19043344447724353\n",
      "Epoch # 7  Loss training:  2.2847073078155518  Accuracy test (mean):  0.9605204127411395  Standard deviation:  0.19473302095107162\n",
      "Epoch # 8  Loss training:  2.618330478668213  Accuracy test (mean):  0.9596231493943472  Standard deviation:  0.1968414604213796\n",
      "Epoch # 9  Loss training:  3.423551082611084  Accuracy test (mean):  0.9632122027815164  Standard deviation:  0.1882404185989166\n",
      "D = 100\n",
      "Final average accuracy (mean):  0.9632122027815164 Final standard deviation:  0.1882404185989166\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing and dataset selection based on the train_test_split method using 40% for test size and 80% for train size.\n",
    "# train_test_split returns random partitions for training and testing.\n",
    "loader_training, loader_test = load_process_data(preprocesar_documento_1, test_size=0.4, train_size=0.6)\n",
    "model = LSTM_TweetClassifier(hidden_dim=100)\n",
    "train_model(model, EPOCHS)\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"D = 100\")\n",
    "print(\"Final average accuracy (mean): \", accuracies.mean(), \"Final standard deviation: \", accuracies.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tabla de resumen:\n",
    "\n",
    "<table>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>~</td>\n",
    "      <td></td>\n",
    "      <td>D = 20</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>D = 100</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Epoc</td>\n",
    "      <td></td>\n",
    "      <td>Accuracy</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>Accuracy</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td></td>\n",
    "      <td>Loss</td>\n",
    "      <td>Mean</td>\n",
    "      <td>Std</td>\n",
    "      <td>Loss</td>\n",
    "      <td>Mean</td>\n",
    "      <td>Std</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>1</td>\n",
    "      <td>15,56038952</td>\n",
    "      <td>0,956034096</td>\n",
    "      <td>0,205019275</td>\n",
    "      <td>33,43926239</td>\n",
    "      <td>0,946164199</td>\n",
    "      <td>0,225693392</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>2</td>\n",
    "      <td>7,256878376</td>\n",
    "      <td>0,963212203</td>\n",
    "      <td>0,188240419</td>\n",
    "      <td>8</td>\n",
    "      <td>0,960520413</td>\n",
    "      <td>0,194733021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>3</td>\n",
    "      <td>5,014931679</td>\n",
    "      <td>0,962314939</td>\n",
    "      <td>0,190433444</td>\n",
    "      <td>5,922861576</td>\n",
    "      <td>0,964109466</td>\n",
    "      <td>0,186017213</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>4</td>\n",
    "      <td>4,117300034</td>\n",
    "      <td>0,966352624</td>\n",
    "      <td>0,180319798</td>\n",
    "      <td>4,44744873</td>\n",
    "      <td>0,963212203</td>\n",
    "      <td>0,188240419</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>5</td>\n",
    "      <td>3,523610115</td>\n",
    "      <td>0,965455361</td>\n",
    "      <td>0,182623402</td>\n",
    "      <td>3,727796793</td>\n",
    "      <td>0,965006729</td>\n",
    "      <td>0,183762732</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>6</td>\n",
    "      <td>2,524313688</td>\n",
    "      <td>0,966352624</td>\n",
    "      <td>0,180319798</td>\n",
    "      <td>3,369677305</td>\n",
    "      <td>0,956034096</td>\n",
    "      <td>0,205019275</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>7</td>\n",
    "      <td>2,14724946</td>\n",
    "      <td>0,650067295</td>\n",
    "      <td>0,183762732</td>\n",
    "      <td>3,12027812</td>\n",
    "      <td>0,962314939</td>\n",
    "      <td>0,190433444</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>8</td>\n",
    "      <td>2,26207304</td>\n",
    "      <td>0,964558098</td>\n",
    "      <td>0,184893953</td>\n",
    "      <td>2,284707308</td>\n",
    "      <td>0,960520413</td>\n",
    "      <td>0,194733021</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>9</td>\n",
    "      <td>2,22116065</td>\n",
    "      <td>0,966352624</td>\n",
    "      <td>0,180319798</td>\n",
    "      <td>2,618330479</td>\n",
    "      <td>0,959623149</td>\n",
    "      <td>0,19684146</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>10</td>\n",
    "      <td>1,460817218</td>\n",
    "      <td>0,965903993</td>\n",
    "      <td>0,18147581</td>\n",
    "      <td>3,423551083</td>\n",
    "      <td>0,963212203</td>\n",
    "      <td>0,188240419</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Mean</td>\n",
    "      <td></td>\n",
    "      <td>0,965903993</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>0,963212203</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Std</td>\n",
    "      <td></td>\n",
    "      <td>0,188240419</td>\n",
    "      <td></td>\n",
    "      <td></td>\n",
    "      <td>0,188240419</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora bien, para las 10 corridas con $D=20$ y $D=100$ se observa un comportamiento similar al presentado en el punto *B*, pero menos acentuado, por lo que vamos a ver a continuación. Básicamente, y tomando como principal factor de observación la pérdida de cada uno, se evidencia que tener un valor para $D$ mayor, puede ser más difícil de manejar que un valor bajo. Comparando lo obtenido, el valor $15.5604$ obtenido para $D=20$ contrasta con el valor $33.4393$ obtenido para $D=100$. Considerando el hecho de que estos valores altos para $D$ son mas valiosos a largo plazo (es decir, con más *epochs*), se deja ver que en ejecuciones más cortas como las realizadas en este TP, la ejecución con más capas es la que se verá más afectada o desfavorecida. Sin embargo, algo que no se puede observar en esta ejecución, es el hecho de que a largo plazo la ejecución con $D=100$, aunque vaya a ser más complicada y lenta a nivel de tiempo y recursos, terminará por obtener mejores valores para la precisión de predicción. \n",
    "\n",
    "Esto último que se comenta si es posible observarlo medianamente a nivel de *accuracy*, ya que, aún teniendo un balance para la pérdida de $1.4608$ contra $3.4236$ (que marca mejores valores a favor del $D$ más bajo), se encuentra que para la media y desviación estándar del $D=100$ se obtienen valores de precisión muy similares e incluso mayores en ocasiones a los provistos por $D=20$. Esto podría dar indicios de que a mayor cantidad de *epochs* el modelo entrenado para con $D=100$ va a tener mayor efectividad y precisión en sus predicciones, en comparación con un modelo entrenado con $D=20$ que, aunque sea más eficiente al principio (hablando de los valores en *epochs* tempranos de la tabla anterior), se va a ver desplazado por el modelo con más capas."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f5a3f61b010ddbe17d95f61e920b2f5278eab5a0f82f7faab4ab0a0a3807f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
