{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 2: Redes Recurrentes y Representaciones Incrustadas\n",
    "\n",
    "## 2. (30 puntos extra) Perceptr√≥n multi-capa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# classifier imports\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from the SMS+Spam+Collection\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read the dataset using Pandas and delimiter as tabulation.\n",
    "messages = pd.read_csv('.\\smsspamcollection\\SMSSpamCollection', encoding='latin-1',delimiter=\"\\t\",header=None)\n",
    "#Set labels on the colums to ease manipulation.\n",
    "messages.columns = [\"label\", \"text\"]\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Preparing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ham with 0 and spam with 1\n",
    "messages = messages.replace(['ham','spam'],[0, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim implementation for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>text_pp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>[ok, lar, joking, wif, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>[dun, say, so, early, hor, already, then, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text  \\\n",
       "0      0  Go until jurong point, crazy.. Available only ...   \n",
       "1      0                      Ok lar... Joking wif u oni...   \n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      0  U dun say so early hor... U c already then say...   \n",
       "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                             text_pp  \n",
       "0  [go, until, jurong, point, crazy, available, o...  \n",
       "1                        [ok, lar, joking, wif, oni]  \n",
       "2  [free, entry, in, wkly, comp, to, win, fa, cup...  \n",
       "3     [dun, say, so, early, hor, already, then, say]  \n",
       "4  [nah, don, think, he, goes, to, usf, he, lives...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Preprocess with built-in Gensim libraries creating a new column with the new pre-processed text.\n",
    "#simple_preprocess lowercases, tokenizes and de-accents and returns the final tokens as unicode strings.\n",
    "#We are calling the pre-processed text, text_pp.\n",
    "messages['text_pp'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x,deacc=True,min_len=2,max_len=15))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4825\n",
       "1     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count amount of ham and spam.\n",
    "# Where 0 is not spam and 1 spam.\n",
    "messages['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Messages\n",
    "\n",
    "### PorterStemeer to remove stopwords from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an empty list to build the corpus for the word2Vec model.\n",
    "corpus = []\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PorterStemeer to remove stopwords from the dataset and create the corpus.\n",
    "for i in range(0, 5572):\n",
    "\n",
    "    \n",
    "    msg = messages['text_pp'][i]   \n",
    "\n",
    "    # Stemming with PorterStemmer handling Stop Words\n",
    "    msg = [ps.stem(word) for word in msg if not word in set(stopwords.words('english'))]\n",
    "    \n",
    "    # preparing Messages with Remaining Tokens\n",
    "    msg = ' '.join(msg)\n",
    "    \n",
    "    # Preparing WordVector Corpus\n",
    "    corpus.append(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec model\n",
    "\n",
    "### Build the Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The number of features for the model or D, according to TP document.\n",
    "D=100\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(min_count=2,\n",
    "                     window=3,\n",
    "                     vector_size=D,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build word2vec vocabulary with the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.0 mins\n"
     ]
    }
   ],
   "source": [
    "#Build word2vec vocabulary with the complete dataset.\n",
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(messages['text_pp'], progress_per=10000)\n",
    "#Prints the time that it tool to build the vocabulary.\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 0.01 mins\n"
     ]
    }
   ],
   "source": [
    "#Train the word2vec model.\n",
    "t = time()\n",
    "\n",
    "w2v_model.train(messages['text_pp'], total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "#Prints the time that it took to train the model.\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19160908,  0.23981117,  0.4922505 , -0.19098581,  0.1856047 ,\n",
       "       -0.25444582, -0.10798351,  0.3521298 , -0.22197844, -0.22279803,\n",
       "       -0.07858622, -0.58972776, -0.15920813,  0.27845109,  0.16600876,\n",
       "       -0.08639668, -0.03629502, -0.41243225,  0.22971849, -0.35526857,\n",
       "       -0.25310177,  0.1740092 , -0.14529708,  0.22386938, -0.15698647,\n",
       "        0.5630012 , -0.10885131,  0.07877631, -0.3082642 ,  0.09887859,\n",
       "        0.19486746, -0.10290623,  0.16490392, -0.17262237, -0.27110523,\n",
       "        0.2857816 ,  0.0625147 , -0.3017694 , -0.18171684, -0.68487823,\n",
       "       -0.00179522,  0.22696145, -0.50293785,  0.2459044 ,  0.35869107,\n",
       "        0.05961956, -0.335956  , -0.00786543, -0.17862292,  0.07066388,\n",
       "       -0.2635694 , -0.12014701, -0.44177553, -0.01300936, -0.55826044,\n",
       "        0.34145817,  0.5019229 , -0.27082464, -0.04179491,  0.15661003,\n",
       "        0.30356234,  0.3772025 , -0.21901399,  0.13329476,  0.0425786 ,\n",
       "        0.09004112,  0.00775765,  0.08436573, -0.13285293,  0.04827133,\n",
       "       -0.24424356, -0.0846597 ,  0.02579471,  0.10821003,  0.12071999,\n",
       "        0.1082117 ,  0.20302555, -0.16620013, -0.09685103, -0.01705716,\n",
       "       -0.17135042, -0.04118485, -0.1622989 ,  0.06429579,  0.30334675,\n",
       "        0.1394438 , -0.31852353,  0.07122932,  0.3088901 , -0.06764276,\n",
       "        0.21078314,  0.07743206,  0.14414512,  0.02918453,  0.18463081,\n",
       "        0.3439598 , -0.03584324, -0.29142895,  0.04754777,  0.20154236],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can check the vectors from a specific word:\n",
    "w2v_model.wv['now']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lives', 0.9979659914970398),\n",
       " ('ben', 0.9979487657546997),\n",
       " ('jeans', 0.9979398250579834),\n",
       " ('jason', 0.9979063868522644),\n",
       " ('bloody', 0.9979048371315002)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another word2vec test to find similar words.\n",
    "w2v_model.wv.most_similar('film',topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the index from a word.\n",
    "w2v_model.wv.key_to_index[\"film\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction function for point 2.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 MLP implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data iterator\n",
    "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches.\n",
    "\n",
    "Taken from the provided \"Natural_disaster_NLP_LSTM.ipynb\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''  \n",
    "\tdef __init__(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tInits the dataset mapper\n",
    "\t\t\"\"\"\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the length of the dataset\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\"\"\"\n",
    "\t\tFetches a specific item by id\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function extract_features_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of features for the model or D, according to TP document. And is set on previous steps while creating the word2vec model.\n",
    "\n",
    "w2v_model = word2vec.Word2Vec(min_count=2,\n",
    "                     window=3,\n",
    "                     vector_size=D,\n",
    "                     sample=6e-5,\n",
    "                     alpha=0.03,\n",
    "                     min_alpha=0.0007,\n",
    "                     negative=20,\n",
    "                     workers=4)\n",
    "\n",
    "# Function requested on point 2.1, to generate a dataset using a defined amount of features (D).\n",
    "def extract_features_dataset(model, preprocesed_dataset, max_length_words=100, num_features=20,batch_size=64):\n",
    "    \n",
    "    # We use the CountVectorizer to convert the collection of text messages to a matrix of token counts.\n",
    "    cv = CountVectorizer(max_features=num_features)\n",
    "    # And create our x array that will be used later on the MLP model.\n",
    "    x = cv.fit_transform(corpus).toarray()\n",
    "\n",
    "    # We built our y, using the labels from the dataset.\n",
    "    y = messages['label']\n",
    "    # Then transform the labels and prepare y for later use on MLP model.\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "    # Load training data\n",
    "    # Taken from the provided \"Natural_disaster_NLP_LSTM.ipynb\" file.\n",
    "    # We will split the dataset in training and testing sets, will be later feed to the dataloder.\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.20, )\n",
    "\n",
    "    # create data loaders\n",
    "    training_set = DatasetMaper(xtrain, ytrain)\n",
    "    test_set = DatasetMaper(xtest,ytest)\n",
    "    loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "    loader_test = DataLoader(test_set, batch_size=batch_size)\n",
    "    \n",
    "    return loader_training, loader_test\n",
    "\n",
    "#Grabs the dataset after calling the function to built with the desired features.\n",
    "loader_training, loader_test = extract_features_dataset(w2v_model,messages['text_pp'],num_features=D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP model\n",
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=70, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=70, out_features=70, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=70, out_features=2, bias=True)\n",
      "  (5): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def create_MLP_model():\n",
    "    # Model creation with neural net Sequential model\n",
    "    model=nn.Sequential(nn.Linear(D,70), # 1 Layer\n",
    "                        nn.ReLU(),          # Activation function ReLu.\n",
    "                        nn.Linear(70,70),  # 2 Layer\n",
    "                        nn.ReLU(),        # Activation function ReLu.\n",
    "                        nn.Linear(70,2),   # 3 Layer\n",
    "                        nn.Sigmoid() #Output activation function Sigmoid.\n",
    "                      ) \n",
    "    return model\n",
    "\n",
    "#Calls the functions to create the model.\n",
    "mlp_model = create_MLP_model()\n",
    "\n",
    "#Error function, selectec Cross Entropy as per documented in the pdf file.\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "#Prints the details of the model.\n",
    "print(\"MLP model\")\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training MLP model\n",
      "- Epoch 0 - Training loss: 0.5033365990434374\n",
      "- Epoch 1 - Training loss: 0.44918989496571676\n",
      "- Epoch 2 - Training loss: 0.4486369963203158\n",
      "- Epoch 3 - Training loss: 0.4484444060495922\n",
      "- Epoch 4 - Training loss: 0.44834481009415217\n",
      "- Epoch 5 - Training loss: 0.44828164151736666\n",
      "- Epoch 6 - Training loss: 0.44823518991470335\n",
      "- Epoch 7 - Training loss: 0.4481964307171958\n",
      "- Epoch 8 - Training loss: 0.4481601906674249\n",
      "- Epoch 9 - Training loss: 0.44812259461198534\n",
      "\n",
      "Training Time (in minutes) = 0.011097991466522216\n"
     ]
    }
   ],
   "source": [
    "#Training function, using as reference the notebook shared in class.\n",
    "def train_model(model, criterion, epochs = 15, lr = 0.01, is_MLP = False):\n",
    "\n",
    "    time0 = time()    \n",
    "    running_loss_list= []\n",
    "    epochs_list = []\n",
    "    optimizer = optim.SGD(model.parameters(), lr= lr, momentum=0.9)\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        \n",
    "        #go for every batch\n",
    "        for x_batch, y_batch in loader_training:\n",
    "            \n",
    "            x = x_batch.type(torch.FloatTensor)\n",
    "            y = y_batch.type(torch.LongTensor)\n",
    "            \n",
    "            # Flatenning\n",
    "            if(is_MLP):\n",
    "              x = x.view(x.shape[0], -1) \n",
    "            \n",
    "            # defining gradient in each epoch as 0\n",
    "            optimizer.zero_grad()            \n",
    "            # modeling for each text batch\n",
    "            output = model(x)\n",
    "\n",
    "            # calculating the loss\n",
    "            loss = criterion(output, y)\n",
    "\n",
    "            # This is where the model learns by backpropagating\n",
    "            loss.backward()\n",
    "\n",
    "            # And optimizes its weights here\n",
    "            optimizer.step()\n",
    "\n",
    "            # calculating the loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        else:\n",
    "            print(\"- Epoch {} - Training loss: {}\".format(e, running_loss/len(loader_training)))\n",
    "            \n",
    "    print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)\n",
    "    return model\n",
    "\n",
    "print(\"### Training MLP model\")\n",
    "mlp_model = train_model(mlp_model, criterion, epochs = 10, lr = 0.1, is_MLP = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MLP model\n",
      "Messages Tested = 1115\n",
      "True Positive Tests = 968\n",
      "False Positive Tests = 0\n",
      "False Negative Tests = 147\n",
      "\n",
      "Model Accuracy (Average) = 86.8161 %\n",
      "\n",
      "F-1 Scores:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93       968\n",
      "           1       0.00      0.00      0.00       147\n",
      "\n",
      "    accuracy                           0.87      1115\n",
      "   macro avg       0.43      0.50      0.46      1115\n",
      "weighted avg       0.75      0.87      0.81      1115\n",
      "\n",
      "[[968   0]\n",
      " [147   0]]\n",
      "\n",
      "Accuracy of the model on Testing Sample Data: 0.81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcord\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jcord\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jcord\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#Test model function, using as reference the notebook shared in class.\n",
    "def test_model(testloader, model, verbose = True):\n",
    "\n",
    "    from sklearn import metrics\n",
    "\n",
    "    #Variables later used to calculate F1 scores.\n",
    "    correct_rate, false_negative_rate, all_count = 0, 0, 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for x_batch, y_batch in loader_test:\n",
    "\n",
    "      x = x_batch.type(torch.FloatTensor)\n",
    "      y = y_batch.type(torch.LongTensor)\n",
    "\n",
    "      for i in range(len(y)): # Iterate over targets.\n",
    "        text = x[i].view(1, D)\n",
    "        with torch.no_grad():\n",
    "            logps = model(text)\n",
    "        ps = torch.exp(logps)\n",
    "        probab = list(ps.cpu().numpy()[0])\n",
    "        pred_label = probab.index(max(probab)) # Get predcition for current iteration.\n",
    "        true_label = y_batch.cpu().numpy()[i] # Get expected label from current iteration.\n",
    "        true_labels.append(true_label)\n",
    "        predictions.append(pred_label)\n",
    "        if (true_label == pred_label): correct_rate += 1 # Adds to correct_rate if the prediction is correct.\n",
    "        else:\n",
    "          if (pred_label == 0): false_negative_rate += 1 # False negatives count.\n",
    "\n",
    "        all_count += 1\n",
    "\n",
    "    if (verbose):\n",
    "      #Prints summary of the testing.\n",
    "      print(\"Messages Tested =\", all_count)\n",
    "      print(\"True Positive Tests =\", correct_rate)\n",
    "      print(\"False Positive Tests =\", (all_count - correct_rate) - false_negative_rate)\n",
    "      print(\"False Negative Tests =\", false_negative_rate)\n",
    "      print(\"\\nModel Accuracy (Average) =\", np.round((correct_rate/all_count)*100,4),\"%\")\n",
    "\n",
    "    #Printing the F1-Scores using the sklearn metrics.\n",
    "    print(\"\\nF-1 Scores:\")\n",
    "    print(metrics.classification_report(true_labels, predictions))\n",
    "    print(metrics.confusion_matrix(true_labels, predictions))\n",
    "      \n",
    "    # Printing the Overall Accuracy of the model\n",
    "    F1_Score=metrics.f1_score(true_labels, predictions, average='weighted',zero_division=0)\n",
    "    print('\\nAccuracy of the model on Testing Sample Data:', round(F1_Score,2))\n",
    "\n",
    "    return correct_rate, false_negative_rate, all_count\n",
    "\n",
    "#Show tests results.\n",
    "print(\"Testing MLP model\")\n",
    "res = test_model(loader_training, mlp_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5411a088105c78e15883c58bf538f4089f9dad56c8db4bd0204f934052f24494"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
