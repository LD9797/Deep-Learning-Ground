{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2: Redes Recurrentes y Representaciones Incrustadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. (100 puntos) Red LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.4 # Define % del dataset que será utilizado para el dataset de pruebas que se retornará en el método implementado load_process_data\n",
    "MAX_WORDS = 200 # Establece el TOP de palabras que se utilizan en el diccionario de palabras, el tamaño de la representación incrustada.\n",
    "BATCH_SIZE = 64 # Tamaño del batch que se utilizará para el dataset de entrenamiento.\n",
    "COLLECTION_PATH = \".\\\\smsspamcollection\" # Ruta base para encontrar el dataset de pruebas de spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones requeridas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código Provisto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corresponde con el código encontrado en el archivo \"Natural_disaster_NLP_LSTM.ipynb\", del cual se realizaron las modificaciones y consideraciones que fueron mencionadas durante las clases del curso.\n",
    "\n",
    "#### Data mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetMaper(Dataset):\n",
    "    '''\n",
    "    Handles batches of dataset\n",
    "    '''\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Inits the dataset mapper\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Fetches a specific item by id\n",
    "        \"\"\"\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Modelo de entranamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_TweetClassifier(nn.ModuleList):\n",
    "\n",
    "    def __init__(self, batch_size=BATCH_SIZE, hidden_dim=20, lstm_layers=2, max_words=MAX_WORDS):\n",
    "        \"\"\"\n",
    "        param batch_size: batch size for training data\n",
    "        param hidden_dim: number of hidden units used in the LSTM and the Embedding layer\n",
    "        param lstm_layers: number of lstm_layers\n",
    "        param max_words: maximum sentence length\n",
    "        \"\"\"\n",
    "        super(LSTM_TweetClassifier, self).__init__()\n",
    "        # batch size during training\n",
    "        self.batch_size = batch_size\n",
    "        # number of hidden units in the LSTM layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Number of LSTM layers\n",
    "        self.LSTM_layers = lstm_layers\n",
    "        self.input_size = max_words  # embedding dimension\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)  # Para descartar\n",
    "        #  N, D\t\t\t#  hidden_dim -> Determina el tamaño del embedding\n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)  # Aprender la representacion\n",
    "        self.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers,\n",
    "                            batch_first=True)  # Capaz de aprender/olvidar dependiendo de las relaciones.\n",
    "        self.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
    "        self.fc2 = nn.Linear(257, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        param x: model input\n",
    "        \"\"\"\n",
    "        # it starts with noisy estimations of h and c\n",
    "        #  Context y estado\n",
    "        h = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  # \"Contexto\"\n",
    "        c = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  # \"Estado\"\n",
    "        # Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution.\n",
    "        # The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std)\n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "        # print(\"x shape \", x.shape)\n",
    "        # print(\"embedding \", self.embedding)\n",
    "        out = self.embedding(x)\n",
    "        out, (hidden, cell) = self.lstm(out, (h, c))\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        #  Fully connected network para la clasificacion\n",
    "        out = torch.relu_(self.fc1(out[:, -1, :]))\n",
    "        out = self.dropout(out)\n",
    "        # sigmoid activation function\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código base implementado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Preprocesamientos\n",
    "\n",
    "Métodos de preprocesado implementados según indicaciones del punto 1.a del TP2. Esto incluye la implementación de los métodos *preprocesar_documento_1*, *preprocesar_documento_2* y *preprocess_example*. Este último tiene por objetivo presentar la ejecución de los dos primeros métodos citados en la sección de *Resolución de ejercicios* de más adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocesar_documento_1(document, to_string=False):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(document)\n",
    "    word_tokens = [token.lower() for token in word_tokens]\n",
    "    word_tokens = [token for token in word_tokens if token.isalnum()]  # To remove punctuations\n",
    "    filtration = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtration if not to_string else ' '.join(filtration)\n",
    "\n",
    "def preprocesar_documento_2(document, to_string=False):\n",
    "    word_tokens = preprocesar_documento_1(document)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtration = [lemmatizer.lemmatize(word, pos=\"v\") for word in word_tokens]\n",
    "    return filtration if not to_string else ' '.join(filtration)\n",
    "\n",
    "def preprocess_example():\n",
    "    with open(COLLECTION_PATH + \"\\\\SMSSpamCollection\", 'r') as collection:\n",
    "        for line in collection:\n",
    "            d0 = line.replace(\"ham\", \"\").replace(\"spam\", \"\").replace(\"\\n\", \"\").replace(\"\\t\", \"\")\n",
    "            d1 = \"I thought, I thought of thinking of thanking you for the gift\"\n",
    "            d2 = \"She was thinking of going to go and get you a GIFT!\"\n",
    "            \n",
    "            print(\"Testing preprocessing methods with different lines:\\n\\n{}\\n{}\\n{}\".format(print_test(1, d0), print_test(2, d1), print_test(3, d2)))\n",
    "            break\n",
    "\n",
    "def print_test(test, line):\n",
    "    return \"* Test line #{}: {}\\nPreprocess #1: {}\\nPreprocess #2: {}\\n\".format(test, line, preprocesar_documento_1(line, to_string=True), preprocesar_documento_2(line, to_string=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Tokenizer\n",
    "\n",
    "Los métodos *tokens_to_indexes*, *sequence_to_number_combination* y *adapt_to_input_layer* son implementaciones generadas como \"equivalentes\" para las funciones implementadas en el código provisto llamadas *prepare_tokens* y *sequence_to_token*. Su objetivo principal es, en tres pasos bien definidos por método:\n",
    "\n",
    "1. establecer el *diccionario* del TOP *MAX_WORDS* (cuya explicación se encuentra en la sección de *Constantes*) para la posterior definición de representación numérica para las líneas preprocesadas.\n",
    "2. generar la representación equivalente de cada línea según los valores numéricos obtenidos del diccionario obtenido en el punto anterior (se ignoran aquellas palabras que no estén en el diccionario).\n",
    "3. adaptar la representación obtenida del punto anterior al tamaño *MAX_WORDS* para que coincida con la entrada para el entrenamiento de este punto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_indexes(sentences: list) -> dict:\n",
    "    words_count = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in words_count:\n",
    "                words_count[word] += 1\n",
    "            else:\n",
    "                words_count[word] = 1\n",
    "    words_to_list = list(dict(sorted(words_count.items(), key=lambda item: item[1])))\n",
    "    words_to_list.reverse()\n",
    "    top_max_words = words_to_list[0: MAX_WORDS - 1]\n",
    "    index_words = {}\n",
    "    counter = 1\n",
    "    for word in top_max_words:\n",
    "        index_words[word] = counter\n",
    "        counter += 1\n",
    "    return index_words\n",
    "\n",
    "def sequence_to_number_combination(word: list, index_words: dict):\n",
    "    sequence = []\n",
    "    for token in word:\n",
    "        if token in index_words:\n",
    "            sequence.append(index_words[token])\n",
    "    return sequence\n",
    "\n",
    "def adapt_to_input_layer(dataset, input_layer_size=MAX_WORDS):\n",
    "    new_dataset = []\n",
    "    for data in dataset:\n",
    "        zeros_to_add = input_layer_size - len(data)\n",
    "        new_data_list = [0 for zero in range(zeros_to_add)]\n",
    "        new_data_list.extend(data)\n",
    "        new_dataset.append(new_data_list)\n",
    "    return new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Data Loader\n",
    "\n",
    "Este método concentra todo lo discutido anteriormente y termina por generar los datasets de entrenamiento y pruebas por utilizar en la siguiente sección de *Resolución de ejercicios*, más específicamente en los puntos 1.b y 1.c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_process_data(f_prepros):\n",
    "    # Load dataset\n",
    "    dataset_frame = pd.read_csv('smsspamcollection\\\\SMSSpamCollection', delimiter='\\t', header=None)\n",
    "    # Preprocess document\n",
    "    sentences_list = [f_prepros(sentence) for sentence in dataset_frame[1]]\n",
    "    # Add index to every word\n",
    "    words_dictionary = tokens_to_indexes(sentences_list)\n",
    "    # Transform tokens (words) to indexes (numbers)\n",
    "    sentences_list = [sequence_to_number_combination(sentence, words_dictionary) for sentence in sentences_list]\n",
    "    # One-hot tags ham = 0, spam = 1\n",
    "    tags_list = [0 if tag == \"ham\" else 1 for tag in dataset_frame[0]]\n",
    "    # Build train and test datasets\n",
    "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(sentences_list, tags_list, test_size=TEST_SIZE)\n",
    "    # Adapt data to input layer\n",
    "    x_train = adapt_to_input_layer(X_train_raw)\n",
    "    x_test = adapt_to_input_layer(X_test_raw)\n",
    "    training_set = DatasetMaper(x_train, y_train)\n",
    "    test_set = DatasetMaper(x_test, y_test)\n",
    "    loader_training = DataLoader(training_set, batch_size=BATCH_SIZE)\n",
    "    loader_test = DataLoader(test_set)\n",
    "    return loader_training, loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolución de ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección, haciendo uso del código provisto e implementado anteriormente, se dará resolución a los ejercicios propuestos en el punto #1 del TP2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implemente la siguiente arquitectura de una red LSTM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.1 Muestre un ejemplo con una entrada escogida del pre-procesamiento con ambos enfoques, y explique brevemente los posibles efectos de utilizar el segundo enfoque al primero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing preprocessing methods with different lines:\n",
      "\n",
      "* Test line #1: Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n",
      "Preprocess #1: go jurong point crazy available bugis n great world la e buffet cine got amore wat\n",
      "Preprocess #2: go jurong point crazy available bugis n great world la e buffet cine get amore wat\n",
      "\n",
      "* Test line #2: I thought, I thought of thinking of thanking you for the gift\n",
      "Preprocess #1: thought thought thinking thanking gift\n",
      "Preprocess #2: think think think thank gift\n",
      "\n",
      "* Test line #3: She was thinking of going to go and get you a GIFT!\n",
      "Preprocess #1: thinking going go get gift\n",
      "Preprocess #2: think go go get gift\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocess_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**R/** En el primer preprocesamiento se emplea únicamente la eliminación de mayúsculas, signos de puntuación y *stopwords*. Estas últimas, son palabras cuya significancia es mínima en el procesamiento de lenguaje natural por lo que es posible retirarlas sin generar mayor impacto en el entendimiento de las expresiones. Como se puede observar en las anteriores entradas, su funcionamiento consiste en estandarizar la expresión al retirar mayúsculas y la puntuación, además de eliminar preposiciones y palabras que sean entendidas como *stopwords* por *nltk*. Puede observarse en el *test line #1* como la palabra *until* desaparece al ser preprocesada. Entre los ejemplos que se muestran, el *test line #1* elimina palabras como *until*, *only* e *in*, mientras que en el *test line #2* también se retiran *of*, *you*, *for* y *the*. Esto evidentemente reduce el espectro de palabras a tomar en cuenta para el procesamiento.\n",
    "\n",
    "En el segundo preprocesamiento, aparte de aplicar primero lo expuesto anteriormente, se emplea también la *lematización* de la expresión. Esto permite obtener la *raíz* de una palabra a partir de cualquiera de sus variables posibles, tal como se puede observar en los ejemplos anteriores donde *thinking* pasa a su forma base *think*. Además de eso, otras funciones con las que cuenta es reducir las palabras a su forma singular, cambiar el tiempo, entre otros. Por ejemplo, en el *test line #1* se cambia el verbo en pasado *got* por el verbo *get*, en el *test line #2* se cambian las palabras *thought*, *thinking* y *thanking* por *think*, *think* y *thank* y, finalmente, en el *test line #3* se cambian las palabras *thinking* y *going* por *think* y *go*.\n",
    "\n",
    "Los efectos individuales que se identifican para el primer preprocesamiento consiste en estandarizar todas las frases del dataset por utilizar. Esto igualmente mantiene palabras que son iguales pero que pueden estar conjugadas, haciendo que al ser procesadas se interpreten como palabras distintas, afectando el entrenamiento y posterior prueba utilizando frases muy similares pero que estén conjugadas en otros tiempos, solo por dar un ejemplo.\n",
    "\n",
    "Esto precisamente es lo que se busca con la especialización del método #1, es decir, el segundo preprocesamiento. Este no solo aprovecha las virtudes de estandarización del primer preprocesamiento, sino que también incluye la lematización, lo cual reduce aún más la gama de palabras a utilizar para el análisis, produciendo que tanto al entrenar como probar se encuentre que las palabras *get* y *got*, aunque conjugadas en distinto tiempo, corresponden a la misma palabra y tienen un significado y aplicación relativamente similares. El efecto que se identifica es que optimizará la ejecución del entrenamiento del modelo y favorecerá una mayor efectividad al momento de predecir ya que no tomará las conjugaciones y otras variantes de las palabras como palabras completamente distintas, haciendo que ahora se les pueda dar más sentido al interpretar que son la misma palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_process_data(preprocesar_documento_1)\n",
    "load_process_data(preprocesar_documento_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "9f5a3f61b010ddbe17d95f61e920b2f5278eab5a0f82f7faab4ab0a0a3807f6d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
