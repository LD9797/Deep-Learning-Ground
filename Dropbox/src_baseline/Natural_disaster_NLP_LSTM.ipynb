{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDil9mPEzonO"
   },
   "source": [
    "# LSTM for tweet classification \n",
    "\n",
    "## Data preprocessing\n",
    "The class Preprocessing loads the specific dataset and makes the data partitions. It also converts the input text to indices, in order to feed the embedding layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ONuNI1L-zgfP"
   },
   "outputs": [],
   "source": [
    "#https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn\n",
    "#https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch\n",
    "#https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6\n",
    "import pandas as pd\n",
    "from tensorflow import  keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocessing:\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tClass constructor\n",
    "\t\t\"\"\"\n",
    "\t\tself.data = 'natural_disaster.csv'\n",
    "    #maximum length for each sequence, CORRECT \n",
    "\t\tself.max_len = 200\n",
    "    #Maximum number of words in the dictionary\n",
    "\t\tself.max_words = 200\n",
    "\t\t#percentage of test data\n",
    "\t\tself.test_size = 0.2\n",
    "\t\t\n",
    "\tdef load_data(self):\n",
    "\t\t\"\"\"\n",
    "\t\tLoads and splits the data\n",
    "\t\t\"\"\"\n",
    "\t\t#load training and test data\n",
    "\t\tdf = pd.read_csv(self.data)\n",
    "\t  #eliminate unnecesary information from training data\n",
    "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "\t\t#extract input and labels\n",
    "\t\tX = df['text'].values\n",
    "\t\tY = df['target'].values\n",
    "\t\t#create train/test split using sklearn\n",
    "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
    "\t\t\n",
    "\tdef prepare_tokens(self):\n",
    "\t\t\"\"\"\n",
    "\t\tTokenizes the input text\n",
    "\t\t\"\"\"\n",
    "\t\t#tokenize the input text\n",
    "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
    "\t\tself.tokens.fit_on_texts(self.x_train)\n",
    "\n",
    "\tdef sequence_to_token(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tConverts the input sequence of strings to a sequence of integers\n",
    "\t\t\"\"\"\n",
    "\t\t#transform the token list to a sequence of integers\n",
    "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
    "\t  #add padding using the maximum length specified\n",
    "\t\treturn keras.utils.pad_sequences(sequences, maxlen=self.max_len)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8eXly6T15yC"
   },
   "source": [
    "## Model\n",
    "Creates the LSTM model. The hidden state $h$ and cell $c$ are initialized with noise. The LSTM receives the entire sequence of embeddings. \n",
    "An Embedding layer is trained in order to learn the data representations. \n",
    "At the top of the model, a fully connected model is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "C0_IJv5k17zT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_TweetClassifier(nn.ModuleList):\n",
    "\n",
    "\tdef __init__(self, batch_size = 64, hidden_dim = 20, lstm_layers = 2, max_words = 200):\n",
    "\t\t\"\"\"\n",
    "\t\tparam batch_size: batch size for training data\n",
    "\t\tparam hidden_dim: number of hidden units used in the LSTM and the Embedding layer\n",
    "\t\tparam lstm_layers: number of lstm_layers\n",
    "\t\tparam max_words: maximum sentence length\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM_TweetClassifier, self).__init__()\n",
    "\t\t#batch size during training\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\t#number of hidden units in the LSTM layer\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\t#Number of LSTM layers\n",
    "\t\tself.LSTM_layers = lstm_layers\n",
    "\t\tself.input_size = max_words # embedding dimension\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout(0.5)  # Para descartar\n",
    "\t\t\t\t\t\t\t\t\t\t#  N, D\t\t\t#  hidden_dim -> Determina el tama√±o del embedding\n",
    "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0) # Aprender la representacion\n",
    "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)  # Capaz de aprender/olvidar dependiendo de las relaciones.\n",
    "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
    "\t\tself.fc2 = nn.Linear(257, 1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass\n",
    "\t\tparam x: model input\n",
    "\t\t\"\"\"\n",
    "\t\t#it starts with noisy estimations of h and c\n",
    "\t\t#  Context y estado\n",
    "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  #  \"Contexto\"\n",
    "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  #  \"Estado\"\n",
    "\t\t#Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. \n",
    "\t\t#The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std) \n",
    "\t\ttorch.nn.init.xavier_normal_(h)\n",
    "\t\ttorch.nn.init.xavier_normal_(c)\n",
    "\t\t#print(\"x shape \", x.shape)\n",
    "\t\t#print(\"embedding \", self.embedding)\n",
    "\t\tout = self.embedding(x)\n",
    "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
    "\t\tout = self.dropout(out)\n",
    "\n",
    "\t\t#  Fully connected network para la clasificacion\n",
    "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "\t\tout = self.dropout(out)\n",
    "\t  #sigmoid activation function\n",
    "\t\tout = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcylrtgX2Uoi"
   },
   "source": [
    "## Data iterator\n",
    "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "4inivS9X2gka"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''  \n",
    "\tdef __init__(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tInits the dataset mapper\n",
    "\t\t\"\"\"\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the length of the dataset\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\"\"\"\n",
    "\t\tFetches a specific item by id\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.x[idx], self.y[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2-yLs3z4Pr0"
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "y5xrknmL-r0t"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 22>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     18\u001B[0m   loader_test \u001B[38;5;241m=\u001B[39m DataLoader(test_set)\n\u001B[0;32m     19\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m loader_training, loader_test\n\u001B[1;32m---> 22\u001B[0m loader_training, loader_test \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_data_loaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mcreate_data_loaders\u001B[1;34m(batch_size)\u001B[0m\n\u001B[0;32m      2\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m Preprocessing()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#load the data\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[43mpreprocessor\u001B[49m\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#tokenize the text\u001B[39;00m\n\u001B[0;32m      6\u001B[0m preprocessor\u001B[38;5;241m.\u001B[39mprepare_tokens()\n",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36mcreate_data_loaders\u001B[1;34m(batch_size)\u001B[0m\n\u001B[0;32m      2\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m Preprocessing()\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#load the data\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[43mpreprocessor\u001B[49m\u001B[38;5;241m.\u001B[39mload_data()\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#tokenize the text\u001B[39;00m\n\u001B[0;32m      6\u001B[0m preprocessor\u001B[38;5;241m.\u001B[39mprepare_tokens()\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1179\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.SafeCallWrapper.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:620\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1095\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m_pydevd_bundle\\pydevd_cython_win32_38_64.pyx:1053\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_win32_38_64.PyDBFrame.trace_dispatch\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py:169\u001B[0m, in \u001B[0;36mstop\u001B[1;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001B[0m\n\u001B[0;32m    167\u001B[0m     frame \u001B[38;5;241m=\u001B[39m suspend_jupyter(main_debugger, thread, frame, step_cmd)\n\u001B[0;32m    168\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m frame:\n\u001B[1;32m--> 169\u001B[0m         \u001B[43mmain_debugger\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdo_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    170\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1160\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[0;32m   1157\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[0;32m   1159\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\Program Files\\JetBrains\\PyCharm 2022.2\\plugins\\python\\helpers\\pydev\\pydevd.py:1175\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[1;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[0;32m   1174\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[1;32m-> 1175\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1177\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[0;32m   1179\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def create_data_loaders(batch_size = 64):\n",
    "  preprocessor = Preprocessing()\n",
    "  #load the data\n",
    "  preprocessor.load_data()\n",
    "  #tokenize the text\n",
    "  preprocessor.prepare_tokens()\n",
    "  raw_x_train = preprocessor.x_train\n",
    "  raw_x_test = preprocessor.x_test\n",
    "  y_train = preprocessor.y_train\n",
    "  y_test = preprocessor.y_test\n",
    "  #convert sequence of strings to tokens\n",
    "  x_train = preprocessor.sequence_to_token(raw_x_train)\n",
    "  x_test = preprocessor.sequence_to_token(raw_x_test)\n",
    "  #create data loaders\n",
    "  training_set = DatasetMaper(x_train, y_train)\n",
    "  test_set = DatasetMaper(x_test, y_test)\t\t\n",
    "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "  loader_test = DataLoader(test_set)\n",
    "  return loader_training, loader_test\n",
    "\n",
    "\n",
    "loader_training, loader_test = create_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iC7CpEZ6ag0"
   },
   "source": [
    "## Train the model\n",
    "Train the model using the dataset loader for the training partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "ZEY-b_8A6cWc",
    "outputId": "8127c01f-bbee-492d-9bab-5d49e140f426"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#hyper parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "model = LSTM_TweetClassifier()\n",
    "\n",
    "\n",
    "def train_model(model, epochs = 10, learning_rate = 0.01):\n",
    "\n",
    "  # Defines a RMSprop optimizer to update the parameters\n",
    "  optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    loss_dataset = 0\n",
    "    for x_batch, y_batch in loader_training:\n",
    "      #print(\"x_batch \\n \", x_batch)\n",
    "      #print(\"y batch \\n\", y_batch)\n",
    "      x = x_batch.type(torch.LongTensor)\n",
    "      y = y_batch.type(torch.FloatTensor)\n",
    "      # Feed the model the entire sequence and get output \"y_pred\"\n",
    "      y_pred = model(x).flatten()\n",
    "      #print(\"y\\n\", y)\n",
    "      #print(\"y pred \", y_pred)\n",
    "      # Calculate loss\n",
    "      loss = F.binary_cross_entropy(y_pred, y)\n",
    "\n",
    "      # The gradientes are calculated\n",
    "      # i.e. derivates are calculated\n",
    "      loss.backward()\n",
    "      \n",
    "      # Each parameter is updated\n",
    "      # with torch.no_grad():\n",
    "      #     a -= lr * a.grad\n",
    "      #     b -= lr * b.grad\n",
    "      optimizer.step()      \n",
    "      # Take the gradients to zero!\n",
    "      # a.grad.zero_()\n",
    "      # b.grad.zero_()\n",
    "      optimizer.zero_grad()\n",
    "      loss_dataset += loss\n",
    "    accuracies = evaluate_model(model, loader_test)\n",
    "    print(\"Epoch \", epoch, \" Loss training : \", loss_dataset.item(), \" Accuracy test: \", accuracies.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpL54BHUC1hc"
   },
   "source": [
    "## Model evaluation\n",
    "Evaluate the model using the test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3SyOqidC3Wr",
    "outputId": "d3da4d41-bd3a-48c0-a5d4-f189dcbdacfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0  Loss training :  64.8134994506836  Accuracy test:  0.7170059093893631\n",
      "Epoch  1  Loss training :  54.931549072265625  Accuracy test:  0.7452396585686146\n",
      "Epoch  2  Loss training :  51.968475341796875  Accuracy test:  0.7485226526592252\n",
      "Epoch  3  Loss training :  49.04666519165039  Accuracy test:  0.7386736703873933\n",
      "Epoch  4  Loss training :  47.253501892089844  Accuracy test:  0.7445830597504924\n",
      "Epoch  5  Loss training :  45.21089172363281  Accuracy test:  0.7432698621142482\n",
      "Epoch  6  Loss training :  42.87434387207031  Accuracy test:  0.7458962573867367\n",
      "Epoch  7  Loss training :  41.177879333496094  Accuracy test:  0.7399868680236376\n",
      "Epoch  8  Loss training :  39.51948928833008  Accuracy test:  0.7353906762967827\n",
      "Epoch  9  Loss training :  37.45927429199219  Accuracy test:  0.7498358502954695\n",
      "Epoch  10  Loss training :  35.07893753051758  Accuracy test:  0.7439264609323704\n",
      "Epoch  11  Loss training :  33.68336868286133  Accuracy test:  0.7301378857518056\n",
      "Epoch  12  Loss training :  33.1735954284668  Accuracy test:  0.737360472751149\n",
      "Epoch  13  Loss training :  32.82691955566406  Accuracy test:  0.7334208798424163\n",
      "Epoch  14  Loss training :  31.169898986816406  Accuracy test:  0.7386736703873933\n",
      "Epoch  15  Loss training :  32.461700439453125  Accuracy test:  0.7432698621142482\n",
      "Epoch  16  Loss training :  31.983903884887695  Accuracy test:  0.7393302692055155\n",
      "Epoch  17  Loss training :  28.719520568847656  Accuracy test:  0.7406434668417596\n",
      "Epoch  18  Loss training :  27.588594436645508  Accuracy test:  0.7367038739330269\n",
      "Epoch  19  Loss training :  27.053585052490234  Accuracy test:  0.7301378857518056\n",
      "Epoch  20  Loss training :  27.304277420043945  Accuracy test:  0.7406434668417596\n",
      "Epoch  21  Loss training :  26.443965911865234  Accuracy test:  0.7288246881155613\n",
      "Epoch  22  Loss training :  25.26112937927246  Accuracy test:  0.7380170715692712\n",
      "Epoch  23  Loss training :  24.865163803100586  Accuracy test:  0.7432698621142482\n",
      "Epoch  24  Loss training :  24.694528579711914  Accuracy test:  0.7281680892974393\n",
      "Epoch  25  Loss training :  28.44691276550293  Accuracy test:  0.7229152987524623\n",
      "Epoch  26  Loss training :  23.843677520751953  Accuracy test:  0.7248850952068286\n",
      "Epoch  27  Loss training :  25.097143173217773  Accuracy test:  0.7248850952068286\n",
      "Epoch  28  Loss training :  25.752878189086914  Accuracy test:  0.726854891661195\n",
      "Epoch  29  Loss training :  22.45467185974121  Accuracy test:  0.7294812869336835\n",
      "Epoch  30  Loss training :  26.019742965698242  Accuracy test:  0.7143795141168746\n",
      "Epoch  31  Loss training :  22.85236358642578  Accuracy test:  0.7209455022980958\n",
      "Epoch  32  Loss training :  24.192113876342773  Accuracy test:  0.7307944845699278\n",
      "Epoch  33  Loss training :  30.555944442749023  Accuracy test:  0.7281680892974393\n",
      "Epoch  34  Loss training :  25.42818832397461  Accuracy test:  0.7327642810242941\n",
      "Epoch  35  Loss training :  24.320844650268555  Accuracy test:  0.7340774786605384\n",
      "Epoch  36  Loss training :  23.094512939453125  Accuracy test:  0.7321076822061721\n",
      "Epoch  37  Loss training :  24.78049087524414  Accuracy test:  0.7255416940249507\n",
      "Epoch  38  Loss training :  24.311384201049805  Accuracy test:  0.726854891661195\n",
      "Epoch  39  Loss training :  22.588027954101562  Accuracy test:  0.7327642810242941\n",
      "Epoch  40  Loss training :  21.936187744140625  Accuracy test:  0.7196323046618516\n",
      "Epoch  41  Loss training :  24.13304901123047  Accuracy test:  0.7281680892974393\n",
      "Epoch  42  Loss training :  21.542831420898438  Accuracy test:  0.7281680892974393\n",
      "Epoch  43  Loss training :  21.555593490600586  Accuracy test:  0.7261982928430729\n",
      "Epoch  44  Loss training :  22.7367000579834  Accuracy test:  0.7380170715692712\n",
      "Epoch  45  Loss training :  20.45917510986328  Accuracy test:  0.7248850952068286\n",
      "Epoch  46  Loss training :  22.54036521911621  Accuracy test:  0.7294812869336835\n",
      "Epoch  47  Loss training :  19.963054656982422  Accuracy test:  0.7327642810242941\n",
      "Epoch  48  Loss training :  22.183917999267578  Accuracy test:  0.7406434668417596\n",
      "Epoch  49  Loss training :  21.286680221557617  Accuracy test:  0.7294812869336835\n",
      "average accuracy :  0.7294812869336835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_accuray(y_pred, y_gt):\n",
    "  return accuracy_score(y_pred, y_gt)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader_test):\n",
    "\n",
    "  predictions = []\n",
    "  accuracies = []\n",
    "    # The model is turned in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "      # Skipping gradients update\n",
    "  with torch.no_grad():\n",
    "\n",
    "            # Iterate over the DataLoader object\n",
    "    for x_batch, y_batch in loader_test:\n",
    "      #print(\"batch\")\n",
    "      x = x_batch.type(torch.LongTensor)\n",
    "      y = y_batch.type(torch.FloatTensor)\n",
    "      \n",
    "                  # Feed the model\n",
    "      y_pred = model(x)\n",
    "      y_pred = torch.round(y_pred).flatten()\n",
    "      #print(\"y_pred \\n \", y_pred)\n",
    "                  # Save prediction\n",
    "      predictions += list(y_pred.detach().numpy())\n",
    "      acc_batch = accuracy_score(y_pred, y)\n",
    "      accuracies += [acc_batch]\t\t\t\t\n",
    "  return np.array(accuracies)\n",
    "\n",
    "train_model(model, epochs, learning_rate)\n",
    "\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"average accuracy : \", accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
