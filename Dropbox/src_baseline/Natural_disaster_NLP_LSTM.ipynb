{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDil9mPEzonO"
   },
   "source": [
    "# LSTM for tweet classification \n",
    "\n",
    "## Data preprocessing\n",
    "The class Preprocessing loads the specific dataset and makes the data partitions. It also converts the input text to indices, in order to feed the embedding layer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ONuNI1L-zgfP"
   },
   "outputs": [],
   "source": [
    "#https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn\n",
    "#https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch\n",
    "#https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6\n",
    "import pandas as pd\n",
    "from tensorflow import  keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class Preprocessing:\n",
    "\t\n",
    "\tdef __init__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tClass constructor\n",
    "\t\t\"\"\"\n",
    "\t\tself.data = 'SMSSpammod.csv'\n",
    "    #maximum length for each sequence, CORRECT \n",
    "\t\tself.max_len = 200\n",
    "    #Maximum number of words in the dictionary\n",
    "\t\tself.max_words = 200\n",
    "\t\t#percentage of test data\n",
    "\t\tself.test_size = 0.2\n",
    "\t\t\n",
    "\tdef load_data(self):\n",
    "\t\t\"\"\"\n",
    "\t\tLoads and splits the data\n",
    "\t\t\"\"\"\n",
    "\t\t#load training and test data\n",
    "\t\tdf = pd.read_csv(self.data)\n",
    "\t  #eliminate unnecesary information from training data\n",
    "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
    "\t\t#extract input and labels\n",
    "\t\tX = df['text'].values\n",
    "\t\tY = df['target'].values\n",
    "\t\t#create train/test split using sklearn\n",
    "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
    "\t\t\n",
    "\tdef prepare_tokens(self):\n",
    "\t\t\"\"\"\n",
    "\t\tTokenizes the input text\n",
    "\t\t\"\"\"\n",
    "\t\t#tokenize the input text\n",
    "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
    "\t\tself.tokens.fit_on_texts(self.x_train)\n",
    "\n",
    "\tdef sequence_to_token(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tConverts the input sequence of strings to a sequence of integers\n",
    "\t\t\"\"\"\n",
    "\t\t#transform the token list to a sequence of integers\n",
    "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
    "\t  #add padding using the maximum length specified\n",
    "\t\treturn keras.utils.pad_sequences(sequences, maxlen=self.max_len)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8eXly6T15yC"
   },
   "source": [
    "## Model\n",
    "Creates the LSTM model. The hidden state $h$ and cell $c$ are initialized with noise. The LSTM receives the entire sequence of embeddings. \n",
    "An Embedding layer is trained in order to learn the data representations. \n",
    "At the top of the model, a fully connected model is defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C0_IJv5k17zT"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTM_TweetClassifier(nn.ModuleList):\n",
    "\n",
    "\tdef __init__(self, batch_size = 64, hidden_dim = 20, lstm_layers = 2, max_words = 200):\n",
    "\t\t\"\"\"\n",
    "\t\tparam batch_size: batch size for training data\n",
    "\t\tparam hidden_dim: number of hidden units used in the LSTM and the Embedding layer\n",
    "\t\tparam lstm_layers: number of lstm_layers\n",
    "\t\tparam max_words: maximum sentence length\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(LSTM_TweetClassifier, self).__init__()\n",
    "\t\t#batch size during training\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\t#number of hidden units in the LSTM layer\n",
    "\t\tself.hidden_dim = hidden_dim\n",
    "\t\t#Number of LSTM layers\n",
    "\t\tself.LSTM_layers = lstm_layers\n",
    "\t\tself.input_size = max_words # embedding dimension\n",
    "\t\t\n",
    "\t\tself.dropout = nn.Dropout(0.5)  # Para descartar\n",
    "\t\t\t\t\t\t\t\t\t\t#  N, D\t\t\t#  hidden_dim -> Determina el tama√±o del embedding\n",
    "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0) # Aprender la representacion\n",
    "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)  # Capaz de aprender/olvidar dependiendo de las relaciones.\n",
    "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
    "\t\tself.fc2 = nn.Linear(257, 1)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass\n",
    "\t\tparam x: model input\n",
    "\t\t\"\"\"\n",
    "\t\t#it starts with noisy estimations of h and c\n",
    "\t\t#  Context y estado\n",
    "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  #  \"Contexto\"\n",
    "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))  #  \"Estado\"\n",
    "\t\t#Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. \n",
    "\t\t#The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std) \n",
    "\t\ttorch.nn.init.xavier_normal_(h)\n",
    "\t\ttorch.nn.init.xavier_normal_(c)\n",
    "\t\t#print(\"x shape \", x.shape)\n",
    "\t\t#print(\"embedding \", self.embedding)\n",
    "\t\tout = self.embedding(x)\n",
    "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
    "\t\tout = self.dropout(out)\n",
    "\n",
    "\t\t#  Fully connected network para la clasificacion\n",
    "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
    "\t\tout = self.dropout(out)\n",
    "\t  #sigmoid activation function\n",
    "\t\tout = torch.sigmoid(self.fc2(out))\n",
    "\n",
    "\t\treturn out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcylrtgX2Uoi"
   },
   "source": [
    "## Data iterator\n",
    "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4inivS9X2gka"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class DatasetMaper(Dataset):\n",
    "\t'''\n",
    "\tHandles batches of dataset\n",
    "\t'''  \n",
    "\tdef __init__(self, x, y):\n",
    "\t\t\"\"\"\n",
    "\t\tInits the dataset mapper\n",
    "\t\t\"\"\"\n",
    "\t\tself.x = x\n",
    "\t\tself.y = y\n",
    "\t\t\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\"\n",
    "\t\tReturns the length of the dataset\n",
    "\t\t\"\"\"\n",
    "\t\treturn len(self.x)\n",
    "\t\t\n",
    "\tdef __getitem__(self, idx):\n",
    "\t\t\"\"\"\n",
    "\t\tFetches a specific item by id\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.x[idx], self.y[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v2-yLs3z4Pr0"
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "y5xrknmL-r0t"
   },
   "outputs": [],
   "source": [
    "def create_data_loaders(batch_size = 64):\n",
    "  preprocessor = Preprocessing()\n",
    "  #load the data\n",
    "  preprocessor.load_data()\n",
    "  #tokenize the text\n",
    "  preprocessor.prepare_tokens()\n",
    "  raw_x_train = preprocessor.x_train\n",
    "  raw_x_test = preprocessor.x_test\n",
    "  y_train = preprocessor.y_train\n",
    "  y_test = preprocessor.y_test\n",
    "  #convert sequence of strings to tokens\n",
    "  x_train = preprocessor.sequence_to_token(raw_x_train)\n",
    "  x_test = preprocessor.sequence_to_token(raw_x_test)\n",
    "  #create data loaders\n",
    "  training_set = DatasetMaper(x_train, y_train)\n",
    "  test_set = DatasetMaper(x_test, y_test)\t\t\n",
    "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
    "  loader_test = DataLoader(test_set)\n",
    "  return loader_training, loader_test\n",
    "\n",
    "\n",
    "loader_training, loader_test = create_data_loaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iC7CpEZ6ag0"
   },
   "source": [
    "## Train the model\n",
    "Train the model using the dataset loader for the training partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "id": "ZEY-b_8A6cWc",
    "outputId": "8127c01f-bbee-492d-9bab-5d49e140f426"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "#hyper parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 50\n",
    "model = LSTM_TweetClassifier()\n",
    "\n",
    "\n",
    "def train_model(model, epochs = 10, learning_rate = 0.01):\n",
    "\n",
    "  # Defines a RMSprop optimizer to update the parameters\n",
    "  optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    # model in training mode\n",
    "    model.train()\n",
    "    loss_dataset = 0\n",
    "    for x_batch, y_batch in loader_training:\n",
    "      #print(\"x_batch \\n \", x_batch)\n",
    "      #print(\"y batch \\n\", y_batch)\n",
    "      x = x_batch.type(torch.LongTensor)\n",
    "      y = y_batch.type(torch.FloatTensor)\n",
    "      # Feed the model the entire sequence and get output \"y_pred\"\n",
    "      y_pred = model(x).flatten()\n",
    "      #print(\"y\\n\", y)\n",
    "      #print(\"y pred \", y_pred)\n",
    "      # Calculate loss\n",
    "      loss = F.binary_cross_entropy(y_pred, y)\n",
    "\n",
    "      # The gradientes are calculated\n",
    "      # i.e. derivates are calculated\n",
    "      loss.backward()\n",
    "      \n",
    "      # Each parameter is updated\n",
    "      # with torch.no_grad():\n",
    "      #     a -= lr * a.grad\n",
    "      #     b -= lr * b.grad\n",
    "      optimizer.step()      \n",
    "      # Take the gradients to zero!\n",
    "      # a.grad.zero_()\n",
    "      # b.grad.zero_()\n",
    "      optimizer.zero_grad()\n",
    "      loss_dataset += loss\n",
    "    accuracies = evaluate_model(model, loader_test)\n",
    "    print(\"Epoch \", epoch, \" Loss training : \", loss_dataset.item(), \" Accuracy test: \", accuracies.mean())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dpL54BHUC1hc"
   },
   "source": [
    "## Model evaluation\n",
    "Evaluate the model using the test loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g3SyOqidC3Wr",
    "outputId": "d3da4d41-bd3a-48c0-a5d4-f189dcbdacfe"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'type'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[1;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 35>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     32\u001B[0m       accuracies \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m [acc_batch]\t\t\t\t\n\u001B[0;32m     33\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m np\u001B[38;5;241m.\u001B[39marray(accuracies)\n\u001B[1;32m---> 35\u001B[0m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     37\u001B[0m accuracies \u001B[38;5;241m=\u001B[39m evaluate_model(model, loader_test)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maverage accuracy : \u001B[39m\u001B[38;5;124m\"\u001B[39m, accuracies\u001B[38;5;241m.\u001B[39mmean())\n",
      "Input \u001B[1;32mIn [9]\u001B[0m, in \u001B[0;36mtrain_model\u001B[1;34m(model, epochs, learning_rate)\u001B[0m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m x_batch, y_batch \u001B[38;5;129;01min\u001B[39;00m loader_training:\n\u001B[0;32m     22\u001B[0m   \u001B[38;5;66;03m#print(\"x_batch \\n \", x_batch)\u001B[39;00m\n\u001B[0;32m     23\u001B[0m   \u001B[38;5;66;03m#print(\"y batch \\n\", y_batch)\u001B[39;00m\n\u001B[0;32m     24\u001B[0m   x \u001B[38;5;241m=\u001B[39m x_batch\u001B[38;5;241m.\u001B[39mtype(torch\u001B[38;5;241m.\u001B[39mLongTensor)\n\u001B[1;32m---> 25\u001B[0m   y \u001B[38;5;241m=\u001B[39m \u001B[43my_batch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtype\u001B[49m(torch\u001B[38;5;241m.\u001B[39mFloatTensor)\n\u001B[0;32m     26\u001B[0m   \u001B[38;5;66;03m# Feed the model the entire sequence and get output \"y_pred\"\u001B[39;00m\n\u001B[0;32m     27\u001B[0m   y_pred \u001B[38;5;241m=\u001B[39m model(x)\u001B[38;5;241m.\u001B[39mflatten()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'tuple' object has no attribute 'type'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_accuray(y_pred, y_gt):\n",
    "  return accuracy_score(y_pred, y_gt)\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader_test):\n",
    "\n",
    "  predictions = []\n",
    "  accuracies = []\n",
    "    # The model is turned in evaluation mode\n",
    "  model.eval()\n",
    "\n",
    "      # Skipping gradients update\n",
    "  with torch.no_grad():\n",
    "\n",
    "            # Iterate over the DataLoader object\n",
    "    for x_batch, y_batch in loader_test:\n",
    "      #print(\"batch\")\n",
    "      x = x_batch.type(torch.LongTensor)\n",
    "      y = y_batch.type(torch.FloatTensor)\n",
    "      \n",
    "                  # Feed the model\n",
    "      y_pred = model(x)\n",
    "      y_pred = torch.round(y_pred).flatten()\n",
    "      #print(\"y_pred \\n \", y_pred)\n",
    "                  # Save prediction\n",
    "      predictions += list(y_pred.detach().numpy())\n",
    "      acc_batch = accuracy_score(y_pred, y)\n",
    "      accuracies += [acc_batch]\t\t\t\t\n",
    "  return np.array(accuracies)\n",
    "\n",
    "train_model(model, epochs, learning_rate)\n",
    "\n",
    "accuracies = evaluate_model(model, loader_test)\n",
    "print(\"average accuracy : \", accuracies.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
