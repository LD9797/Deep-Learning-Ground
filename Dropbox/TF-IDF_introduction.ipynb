{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\saul1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\saul1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#based on https://towardsdatascience.com/introduction-to-nlp-part-3-tf-idf-explained-cedb1fc1f7dc\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d1:  I thought, I thought of thinking of thanking you for the gift\n",
      "d2:  She was thinking of going to go and get you a GIFT!\n",
      "d1 preprocessed:  ['think', 'think', 'think', 'thank', 'gift']\n",
      "d2 preprocessed:  ['think', 'go', 'go', 'get', 'gift']\n"
     ]
    }
   ],
   "source": [
    "#Documents\n",
    "d1 = 'I thought, I thought of thinking of thanking you for the gift'\n",
    "d2 = 'She was thinking of going to go and get you a GIFT!'\n",
    "print(\"d1: \", d1)\n",
    "print(\"d2: \", d2)\n",
    "\n",
    "def preprocess_text(document):\n",
    "    \"\"\"\n",
    "    Preprocess an entire document\n",
    "    \"\"\"\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(document)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "    \n",
    "    # Remove stopwords, terms\n",
    "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "    return keywords\n",
    "\n",
    "#preprocess the documents\n",
    "d1_preprocessed = preprocess_text(d1)\n",
    "d2_preprocessed = preprocess_text(d2)\n",
    "print(\"d1 preprocessed: \", d1_preprocessed)\n",
    "print(\"d2 preprocessed: \", d2_preprocessed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.2889723007058112\n",
      "  (0, 3)\t0.40614048585175794\n",
      "  (0, 4)\t0.8669169021174337\n",
      "  (1, 0)\t0.4078241041497786\n",
      "  (1, 2)\t0.8156482082995572\n",
      "  (1, 1)\t0.29017020899133733\n",
      "  (1, 4)\t0.29017020899133733\n",
      "sparse X_train\n",
      "          0         1         2        3         4\n",
      "0  0.000000  0.288972  0.000000  0.40614  0.866917\n",
      "1  0.407824  0.290170  0.815648  0.00000  0.290170\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def display_tfidfs(X_train_vectorised):\n",
    "    # Convert sparse matrix to dataframe\n",
    "    X_train = pd.DataFrame.sparse.from_spmatrix(X_train_vectorised)\n",
    "    print(\"sparse X_train\")\n",
    "    print(X_train)\n",
    "    # Save mapping on which index refers to which words\n",
    "    #col_map = {v:k for k, v in X_train_vectorised.vocabulary_.items()}\n",
    "    # Rename each column using the mapping\n",
    "    #for col in X_train.columns:\n",
    "        #X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
    "    #print(X_train)\n",
    "    \n",
    "\n",
    "\n",
    "# Create an instance of TfidfVectorizer\n",
    "# we can send the preprocessing function as part of the tfidf vectoriser\n",
    "tfidf_vectoriser = TfidfVectorizer(analyzer = preprocess_text)\n",
    "# Create dataframe, input of the Tfidf vectoriser\n",
    "X_train = pd.DataFrame({'corpus': [d1, d2]})\n",
    "\n",
    "\n",
    "# Vectorise the data using the TF-IDF\n",
    "#The result is encoded in a sparse matrix (i.e, 0 values are not included)\n",
    "X_train_vectorised = tfidf_vectoriser.fit_transform(X_train['corpus'])\n",
    "print(X_train_vectorised)\n",
    "display_tfidfs(X_train_vectorised)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
