{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDil9mPEzonO"
      },
      "source": [
        "# LSTM for tweet classification \n",
        "\n",
        "## Data preprocessing\n",
        "The class Preprocessing loads the specific dataset and makes the data partitions. It also converts the input text to indices, in order to feed the embedding layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONuNI1L-zgfP"
      },
      "outputs": [],
      "source": [
        "#https://www.simplilearn.com/tutorials/deep-learning-tutorial/rnn\n",
        "#https://github.com/FernandoLpz/Text-Classification-LSTMs-PyTorch\n",
        "#https://towardsdatascience.com/text-classification-with-pytorch-7111dae111a6\n",
        "import pandas as pd\n",
        "from tensorflow import  keras\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class Preprocessing:\n",
        "\t\n",
        "\tdef __init__(self):\n",
        "\t\t\"\"\"\n",
        "\t\tClass constructor\n",
        "\t\t\"\"\"\n",
        "\t\tself.data = 'natural_disaster.csv'\n",
        "    #maximum length for each sequence, CORRECT \n",
        "\t\tself.max_len = 200\n",
        "    #Maximum number of words in the dictionary\n",
        "\t\tself.max_words = 200\n",
        "\t\t#percentage of test data\n",
        "\t\tself.test_size = 0.2\n",
        "\t\t\n",
        "\tdef load_data(self):\n",
        "\t\t\"\"\"\n",
        "\t\tLoads and splits the data\n",
        "\t\t\"\"\"\n",
        "\t\t#load training and test data\n",
        "\t\tdf = pd.read_csv(self.data)\n",
        "\t  #eliminate unnecesary information from training data\n",
        "\t\tdf.drop(['id','keyword','location'], axis=1, inplace=True)\n",
        "\t\t#extract input and labels\n",
        "\t\tX = df['text'].values\n",
        "\t\tY = df['target'].values\n",
        "\t\t#create train/test split using sklearn\n",
        "\t\tself.x_train, self.x_test, self.y_train, self.y_test = train_test_split(X, Y, test_size=self.test_size)\n",
        "\t\t\n",
        "\tdef prepare_tokens(self):\n",
        "\t\t\"\"\"\n",
        "\t\tTokenizes the input text\n",
        "\t\t\"\"\"\n",
        "\t\t#tokenize the input text\n",
        "\t\tself.tokens = Tokenizer(num_words=self.max_words)\n",
        "\t\tself.tokens.fit_on_texts(self.x_train)\n",
        "\n",
        "\tdef sequence_to_token(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tConverts the input sequence of strings to a sequence of integers\n",
        "\t\t\"\"\"\n",
        "\t\t#transform the token list to a sequence of integers\n",
        "\t\tsequences = self.tokens.texts_to_sequences(x)\n",
        "\t  #add padding using the maximum length specified\n",
        "\t\treturn keras.utils.pad_sequences(sequences, maxlen=self.max_len)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8eXly6T15yC"
      },
      "source": [
        "## Model\n",
        "Creates the LSTM model. The hidden state $h$ and cell $c$ are initialized with noise. The LSTM receives the entire sequence of embeddings. \n",
        "An Embedding layer is trained in order to learn the data representations. \n",
        "At the top of the model, a fully connected model is defined. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0_IJv5k17zT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LSTM_TweetClassifier(nn.ModuleList):\n",
        "\n",
        "\tdef __init__(self, batch_size = 64, hidden_dim = 20, lstm_layers = 2, max_words = 200):\n",
        "\t\t\"\"\"\n",
        "\t\tparam batch_size: batch size for training data\n",
        "\t\tparam hidden_dim: number of hidden units used in the LSTM and the Embedding layer\n",
        "\t\tparam lstm_layers: number of lstm_layers\n",
        "\t\tparam max_words: maximum sentence length\n",
        "\t\t\"\"\"\n",
        "\t\tsuper(LSTM_TweetClassifier, self).__init__()\n",
        "\t\t#batch size during training\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\t#number of hidden units in the LSTM layer\n",
        "\t\tself.hidden_dim = hidden_dim\n",
        "\t\t#Number of LSTM layers\n",
        "\t\tself.LSTM_layers = lstm_layers\n",
        "\t\tself.input_size = max_words # embedding dimension\n",
        "\t\t\n",
        "\t\tself.dropout = nn.Dropout(0.5)\n",
        "\t\tself.embedding = nn.Embedding(self.input_size, self.hidden_dim, padding_idx=0)\n",
        "\t\tself.lstm = nn.LSTM(input_size=self.hidden_dim, hidden_size=self.hidden_dim, num_layers=self.LSTM_layers, batch_first=True)\n",
        "\t\tself.fc1 = nn.Linear(in_features=self.hidden_dim, out_features=257)\n",
        "\t\tself.fc2 = nn.Linear(257, 1)\n",
        "\t\t\n",
        "\tdef forward(self, x):\n",
        "\t\t\"\"\"\n",
        "\t\tForward pass\n",
        "\t\tparam x: model input\n",
        "\t\t\"\"\"\n",
        "\t\t#it starts with noisy estimations of h and c\n",
        "\t\th = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\tc = torch.zeros((self.LSTM_layers, x.size(0), self.hidden_dim))\n",
        "\t\t#Fills the input Tensor with values according to the method described in Understanding the difficulty of training deep feedforward neural networks - Glorot, X. & Bengio, Y. (2010), using a normal distribution. \n",
        "\t\t#The resulting tensor will have values sampled from \\mathcal{N}(0, \\text{std}^2)N(0,std) \n",
        "\t\ttorch.nn.init.xavier_normal_(h)\n",
        "\t\ttorch.nn.init.xavier_normal_(c)\n",
        "\t\t#print(\"x shape \", x.shape)\n",
        "\t\t#print(\"embedding \", self.embedding)\n",
        "\t\tout = self.embedding(x)\n",
        "\t\tout, (hidden, cell) = self.lstm(out, (h,c))\n",
        "\t\tout = self.dropout(out)\n",
        "\t\tout = torch.relu_(self.fc1(out[:,-1,:]))\n",
        "\t\tout = self.dropout(out)\n",
        "\t  #sigmoid activation function\n",
        "\t\tout = torch.sigmoid(self.fc2(out))\n",
        "\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcylrtgX2Uoi"
      },
      "source": [
        "## Data iterator\n",
        "In order to get ready the training phase, first, we need to prepare the way how the sequences will be fed to the model. For this purpose, PyTorch provides two very useful classes: Dataset and DataLoader. The aim of Dataset class is to provide an easy way to iterate over a dataset by batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4inivS9X2gka"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class DatasetMaper(Dataset):\n",
        "\t'''\n",
        "\tHandles batches of dataset\n",
        "\t'''  \n",
        "\tdef __init__(self, x, y):\n",
        "\t\t\"\"\"\n",
        "\t\tInits the dataset mapper\n",
        "\t\t\"\"\"\n",
        "\t\tself.x = x\n",
        "\t\tself.y = y\n",
        "\t\t\n",
        "\tdef __len__(self):\n",
        "\t\t\"\"\"\n",
        "\t\tReturns the length of the dataset\n",
        "\t\t\"\"\"\n",
        "\t\treturn len(self.x)\n",
        "\t\t\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\t\"\"\"\n",
        "\t\tFetches a specific item by id\n",
        "\t\t\"\"\"\n",
        "\t\treturn self.x[idx], self.y[idx]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2-yLs3z4Pr0"
      },
      "source": [
        "## Load training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5xrknmL-r0t"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(batch_size = 64):\n",
        "  preprocessor = Preprocessing()\n",
        "  #load the data\n",
        "  preprocessor.load_data()\n",
        "  #tokenize the text\n",
        "  preprocessor.prepare_tokens()\n",
        "  raw_x_train = preprocessor.x_train\n",
        "  raw_x_test = preprocessor.x_test\n",
        "  y_train = preprocessor.y_train\n",
        "  y_test = preprocessor.y_test\n",
        "  #convert sequence of strings to tokens\n",
        "  x_train = preprocessor.sequence_to_token(raw_x_train)\n",
        "  x_test = preprocessor.sequence_to_token(raw_x_test)\n",
        "  #create data loaders\n",
        "  training_set = DatasetMaper(x_train, y_train)\n",
        "  test_set = DatasetMaper(x_test, y_test)\t\t\n",
        "  loader_training = DataLoader(training_set, batch_size=batch_size)\n",
        "  loader_test = DataLoader(test_set)\n",
        "  return loader_training, loader_test\n",
        "\n",
        "\n",
        "loader_training, loader_test = create_data_loaders()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iC7CpEZ6ag0"
      },
      "source": [
        "## Train the model\n",
        "Train the model using the dataset loader for the training partition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "ZEY-b_8A6cWc",
        "outputId": "8127c01f-bbee-492d-9bab-5d49e140f426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  0  Loss training :  65.32823944091797  Accuracy test:  0.7019041365725541\n",
            "Epoch  1  Loss training :  54.60485076904297  Accuracy test:  0.7248850952068286\n",
            "Epoch  2  Loss training :  50.324092864990234  Accuracy test:  0.7367038739330269\n",
            "Epoch  3  Loss training :  48.17243957519531  Accuracy test:  0.7406434668417596\n",
            "Epoch  4  Loss training :  46.214290618896484  Accuracy test:  0.7347340774786605\n",
            "Epoch  5  Loss training :  44.86140060424805  Accuracy test:  0.7248850952068286\n",
            "Epoch  6  Loss training :  42.73770523071289  Accuracy test:  0.7439264609323704\n",
            "Epoch  7  Loss training :  40.08542251586914  Accuracy test:  0.7367038739330269\n",
            "Epoch  8  Loss training :  44.32766342163086  Accuracy test:  0.7432698621142482\n",
            "Epoch  9  Loss training :  42.178916931152344  Accuracy test:  0.7393302692055155\n",
            "Epoch  10  Loss training :  43.74140167236328  Accuracy test:  0.7091267235718975\n",
            "Epoch  11  Loss training :  38.708675384521484  Accuracy test:  0.747209455022981\n",
            "Epoch  12  Loss training :  37.35714340209961  Accuracy test:  0.7439264609323704\n",
            "Epoch  13  Loss training :  35.551490783691406  Accuracy test:  0.7452396585686146\n",
            "Epoch  14  Loss training :  35.20051193237305  Accuracy test:  0.7432698621142482\n",
            "Epoch  15  Loss training :  34.184364318847656  Accuracy test:  0.7432698621142482\n",
            "Epoch  16  Loss training :  34.90276336669922  Accuracy test:  0.7255416940249507\n",
            "Epoch  17  Loss training :  35.26191329956055  Accuracy test:  0.7334208798424163\n",
            "Epoch  18  Loss training :  32.192893981933594  Accuracy test:  0.7347340774786605\n",
            "Epoch  19  Loss training :  31.774871826171875  Accuracy test:  0.7275114904793172\n",
            "Epoch  20  Loss training :  32.33707046508789  Accuracy test:  0.7360472751149048\n",
            "Epoch  21  Loss training :  32.55669021606445  Accuracy test:  0.7301378857518056\n",
            "Epoch  22  Loss training :  31.232505798339844  Accuracy test:  0.7393302692055155\n",
            "Epoch  23  Loss training :  29.458505630493164  Accuracy test:  0.7399868680236376\n",
            "Epoch  24  Loss training :  29.99106788635254  Accuracy test:  0.7406434668417596\n",
            "Epoch  25  Loss training :  29.646705627441406  Accuracy test:  0.7399868680236376\n",
            "Epoch  26  Loss training :  28.178768157958984  Accuracy test:  0.7406434668417596\n",
            "Epoch  27  Loss training :  30.28880500793457  Accuracy test:  0.7281680892974393\n",
            "Epoch  28  Loss training :  30.569950103759766  Accuracy test:  0.7281680892974393\n",
            "Epoch  29  Loss training :  28.713729858398438  Accuracy test:  0.7347340774786605\n",
            "Epoch  30  Loss training :  27.52449607849121  Accuracy test:  0.7301378857518056\n",
            "Epoch  31  Loss training :  26.942264556884766  Accuracy test:  0.7340774786605384\n",
            "Epoch  32  Loss training :  30.590166091918945  Accuracy test:  0.7321076822061721\n",
            "Epoch  33  Loss training :  27.609533309936523  Accuracy test:  0.7242284963887065\n",
            "Epoch  34  Loss training :  26.678775787353516  Accuracy test:  0.7189757058437295\n",
            "Epoch  35  Loss training :  26.713333129882812  Accuracy test:  0.7353906762967827\n",
            "Epoch  36  Loss training :  26.72419548034668  Accuracy test:  0.7347340774786605\n",
            "Epoch  37  Loss training :  26.945589065551758  Accuracy test:  0.737360472751149\n",
            "Epoch  38  Loss training :  28.842214584350586  Accuracy test:  0.7334208798424163\n",
            "Epoch  39  Loss training :  23.514802932739258  Accuracy test:  0.7248850952068286\n",
            "Epoch  40  Loss training :  26.459217071533203  Accuracy test:  0.7360472751149048\n",
            "Epoch  41  Loss training :  25.834735870361328  Accuracy test:  0.7386736703873933\n",
            "Epoch  42  Loss training :  25.461496353149414  Accuracy test:  0.7380170715692712\n",
            "Epoch  43  Loss training :  27.329776763916016  Accuracy test:  0.7255416940249507\n",
            "Epoch  44  Loss training :  23.594593048095703  Accuracy test:  0.7294812869336835\n",
            "Epoch  45  Loss training :  22.151830673217773  Accuracy test:  0.7380170715692712\n",
            "Epoch  46  Loss training :  24.039337158203125  Accuracy test:  0.7360472751149048\n",
            "Epoch  47  Loss training :  23.654096603393555  Accuracy test:  0.7386736703873933\n",
            "Epoch  48  Loss training :  24.89255714416504  Accuracy test:  0.7255416940249507\n",
            "Epoch  49  Loss training :  23.80832862854004  Accuracy test:  0.7294812869336835\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "#hyper parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 50\n",
        "model = LSTM_TweetClassifier()\n",
        "\n",
        "\n",
        "def train_model(model, epochs = 10, learning_rate = 0.01):\n",
        "\n",
        "  # Defines a RMSprop optimizer to update the parameters\n",
        "  optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # model in training mode\n",
        "    model.train()\n",
        "    loss_dataset = 0\n",
        "    for x_batch, y_batch in loader_training:\n",
        "      #print(\"x_batch \\n \", x_batch)\n",
        "      #print(\"y batch \\n\", y_batch)\n",
        "      x = x_batch.type(torch.LongTensor)\n",
        "      y = y_batch.type(torch.FloatTensor)\n",
        "      # Feed the model the entire sequence and get output \"y_pred\"\n",
        "      y_pred = model(x).flatten()\n",
        "      #print(\"y\\n\", y)\n",
        "      #print(\"y pred \", y_pred)\n",
        "      # Calculate loss\n",
        "      loss = F.binary_cross_entropy(y_pred, y)\n",
        "\n",
        "      # The gradientes are calculated\n",
        "      # i.e. derivates are calculated\n",
        "      loss.backward()\n",
        "      \n",
        "      # Each parameter is updated\n",
        "      # with torch.no_grad():\n",
        "      #     a -= lr * a.grad\n",
        "      #     b -= lr * b.grad\n",
        "      optimizer.step()      \n",
        "      # Take the gradients to zero!\n",
        "      # a.grad.zero_()\n",
        "      # b.grad.zero_()\n",
        "      optimizer.zero_grad()\n",
        "      loss_dataset += loss\n",
        "    accuracies = evaluate_model(model, loader_test)\n",
        "    print(\"Epoch \", epoch, \" Loss training : \", loss_dataset.item(), \" Accuracy test: \", accuracies.mean())\n",
        "\n",
        "train_model(model, epochs, learning_rate)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpL54BHUC1hc"
      },
      "source": [
        "## Model evaluation\n",
        "Evaluate the model using the test loader."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3SyOqidC3Wr",
        "outputId": "d3da4d41-bd3a-48c0-a5d4-f189dcbdacfe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "average accuracy :  0.5843729481286933\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def calculate_accuray(y_pred, y_gt):\n",
        "  return accuracy_score(y_pred, y_gt)\n",
        "\n",
        "\n",
        "def evaluate_model(model, loader_test):\n",
        "\n",
        "  predictions = []\n",
        "  accuracies = []\n",
        "    # The model is turned in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "      # Skipping gradients update\n",
        "  with torch.no_grad():\n",
        "\n",
        "            # Iterate over the DataLoader object\n",
        "    for x_batch, y_batch in loader_test:\n",
        "      #print(\"batch\")\n",
        "      x = x_batch.type(torch.LongTensor)\n",
        "      y = y_batch.type(torch.FloatTensor)\n",
        "      \n",
        "                  # Feed the model\n",
        "      y_pred = model(x)\n",
        "      y_pred = torch.round(y_pred).flatten()\n",
        "      #print(\"y_pred \\n \", y_pred)\n",
        "                  # Save prediction\n",
        "      predictions += list(y_pred.detach().numpy())\n",
        "      acc_batch = accuracy_score(y_pred, y)\n",
        "      accuracies += [acc_batch]\t\t\t\t\n",
        "  return np.array(accuracies)\n",
        "\n",
        "\n",
        "accuracies = evaluate_model(model, loader_test)\n",
        "print(\"average accuracy : \", accuracies.mean())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}